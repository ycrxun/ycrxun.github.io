[{"content":"","date":"2024-06-21T13:44:30+08:00","permalink":"https://ronggle.com/2024/memoirs-at-2024/","title":"回顾过去这几年"},{"content":"什么是区块链？ 从纯技术的角度来看，就是将一个个数据区块按照一定的顺序，将其连接起来；从业务现象来看，区块链网络的核心是就是分布式账本，记录了网络上发生的所有交易。\n区块链的特性 区中心化（分散式） 通常被描述为去中心化的，因为每一个参与者都具有相同的账本信息。\n协作 每个参与者共同协作来维护它，不是单一参与者说了算。\n不可篡改 区块链被设计为只可以追加，使用了加密技术来保证一旦将交易添加到账本中，就不能对其进行修改。\n什么是Hyperledger Fabric？ Linux基金会于2015年创建了Hyperledger项目，以推进跨行业的区块链技术。Hyperledger Fabric是Hyperledger中的区块链项目之一。像其他区块链技术一样，它具有分类帐，使用智能合约，并且是参与者用来管理其交易的系统。Hyperledger Fabric与其他区块链系统最大的不同体现在私有和许可。 Hyperledger Fabric是一个（平台），提供分布式账本解决方案的平台，Hyperledger Fabric由模块化架构支撑，并具备极佳的保密性、可伸缩性、灵活性、可扩展性，Hyperledger Fabric被设计成（模块直接插拔，适用多种场景），支持不同的模块组件直接拔插启用，并能适应在经济生态系统中错综复杂的各种场景。\nHyperledger Fabric的交易流程 更多关于Hyperledger Fabric的内容，可以查看官方文档以及官方文档翻译\n怎么使用Hyperledger Fabric 以下内容基本上是交流时进行的实践步骤中涉及的配置和脚本，如果你想快捷体验，可以传送至后面\n准备阶段 Docker Docker Swarm Hyperledger Fabric镜像 Hyperledger Fabric镜像 Docker安装，以及Docker Swarm初始化，如果你还不了解，那你可以先自行多吉或者必应一下。\n这里我们需要拉取一下Hyperledger Fabric镜像，由于在2.0之前，Hyperledger Fabric镜像的体积都比较大，拉取比较慢，虽然现在2.0还处于alpha阶段，但是我们搭建一个是开发网络，是没有什么影响的。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 #!/usr/bin/env bash clear FABRIC_TAG=2.0 function pullImages() { for image in peer orderer ccenv tools ca; do echo \u0026#34;Pull image hyperledger/fabric-$image:$FABRIC_TAG\u0026#34; docker pull hyperledger/fabric-\u0026#34;$image\u0026#34;:\u0026#34;$FABRIC_TAG\u0026#34; sleep 1 done } echo \u0026#34;Pull images for hyperledger fabric network.\u0026#34; pullImages 创建Docker网络 1 2 3 4 5 6 7 #!/usr/bin/env bash clear DOCKER_NETWORK_NAME=baas echo \u0026#34;Create docker network $DOCKER_NETWORK_NAME\u0026#34; docker network create --driver overlay --subnet=10.200.1.0/24 --attachable \u0026#34;$DOCKER_NETWORK_NAME\u0026#34; 准备Hyperledger Fabric网络配置 网络成员加密材料配置 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 # --------------------------------------------------------------------------- # \u0026#34;OrdererOrgs\u0026#34; - Definition of organizations managing orderer nodes # --------------------------------------------------------------------------- OrdererOrgs: # --------------------------------------------------------------------------- # Orderer # --------------------------------------------------------------------------- - Name: Orderer Domain: cloud-labs.io CA: Country: CN Province: Chongqing Locality: Chongqing # --------------------------------------------------------------------------- # \u0026#34;Specs\u0026#34; - See PeerOrgs below for complete description # --------------------------------------------------------------------------- Specs: - Hostname: orderer0 - Hostname: orderer1 - Hostname: orderer2 - Hostname: orderer3 - Hostname: orderer4 # --------------------------------------------------------------------------- # \u0026#34;PeerOrgs\u0026#34; - Definition of organizations managing peer nodes # --------------------------------------------------------------------------- PeerOrgs: # --------------------------------------------------------------------------- # storage org for cloud-labs.io # --------------------------------------------------------------------------- - Name: storage Domain: storage.cloud-labs.io EnableNodeOUs: true CA: Country: CN Province: Chongqing Locality: Chongqing Template: Count: 1 Users: Count: 1 # --------------------------------------------------------------------------- # vhost org for cloud-labs.io # --------------------------------------------------------------------------- - Name: vhost Domain: vhost.cloud-labs.io EnableNodeOUs: true CA: Country: CN Province: Chongqing Locality: Chongqing Template: Count: 1 Users: Count: 1 网络初始化以及通道配置 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 --- Organizations: - \u0026amp;cloudLabs Name: cloudLabs ID: cloudLabsMSP MSPDir: ./artifacts/crypto-config/ordererOrganizations/cloud-labs.io/msp Policies: Readers: Type: Signature Rule: \u0026#34;OR(\u0026#39;cloudLabsMSP.member\u0026#39;)\u0026#34; Writers: Type: Signature Rule: \u0026#34;OR(\u0026#39;cloudLabsMSP.member\u0026#39;)\u0026#34; Admins: Type: Signature Rule: \u0026#34;OR(\u0026#39;cloudLabsMSP.admin\u0026#39;)\u0026#34; - \u0026amp;storage Name: storage ID: storageMSP MSPDir: ./artifacts/crypto-config/peerOrganizations/storage.cloud-labs.io/msp Policies: Readers: Type: Signature Rule: \u0026#34;OR(\u0026#39;storageMSP.admin\u0026#39;, \u0026#39;storageMSP.peer\u0026#39;, \u0026#39;storageMSP.client\u0026#39;)\u0026#34; Writers: Type: Signature Rule: \u0026#34;OR(\u0026#39;storageMSP.admin\u0026#39;, \u0026#39;storageMSP.client\u0026#39;)\u0026#34; Admins: Type: Signature Rule: \u0026#34;OR(\u0026#39;storageMSP.admin\u0026#39;)\u0026#34; AnchorPeers: - Host: peer0.storage.cloud-labs.io Port: 7051 - \u0026amp;vhost Name: vhost ID: vhostMSP MSPDir: ./artifacts/crypto-config/peerOrganizations/vhost.cloud-labs.io/msp Policies: Readers: Type: Signature Rule: \u0026#34;OR(\u0026#39;vhostMSP.admin\u0026#39;, \u0026#39;vhostMSP.peer\u0026#39;, \u0026#39;vhostMSP.client\u0026#39;)\u0026#34; Writers: Type: Signature Rule: \u0026#34;OR(\u0026#39;vhostMSP.admin\u0026#39;, \u0026#39;vhostMSP.client\u0026#39;)\u0026#34; Admins: Type: Signature Rule: \u0026#34;OR(\u0026#39;vhostMSP.admin\u0026#39;)\u0026#34; AnchorPeers: - Host: peer0.vhost.cloud-labs.io Port: 7051 Capabilities: Channel: \u0026amp;ChannelCapabilities V1_3: true Orderer: \u0026amp;OrdererCapabilities V1_1: true Application: \u0026amp;ApplicationCapabilities V1_3: true V1_2: false V1_1: false Application: \u0026amp;ApplicationDefaults Organizations: Policies: Readers: Type: ImplicitMeta Rule: \u0026#34;ANY Readers\u0026#34; Writers: Type: ImplicitMeta Rule: \u0026#34;ANY Writers\u0026#34; Admins: Type: ImplicitMeta Rule: \u0026#34;MAJORITY Admins\u0026#34; Capabilities: \u0026lt;\u0026lt;: *ApplicationCapabilities Orderer: \u0026amp;OrdererDefaults OrdererType: etcdraft # solo, kafka EtcdRaft: Consenters: - Host: orderer0.cloud-labs.io Port: 7050 ClientTLSCert: ./artifacts/crypto-config/ordererOrganizations/cloud-labs.io/orderers/orderer0.cloud-labs.io/tls/server.crt ServerTLSCert: ./artifacts/crypto-config/ordererOrganizations/cloud-labs.io/orderers/orderer0.cloud-labs.io/tls/server.crt - Host: orderer1.cloud-labs.io Port: 7050 ClientTLSCert: ./artifacts/crypto-config/ordererOrganizations/cloud-labs.io/orderers/orderer1.cloud-labs.io/tls/server.crt ServerTLSCert: ./artifacts/crypto-config/ordererOrganizations/cloud-labs.io/orderers/orderer1.cloud-labs.io/tls/server.crt - Host: orderer2.cloud-labs.io Port: 7050 ClientTLSCert: ./artifacts/crypto-config/ordererOrganizations/cloud-labs.io/orderers/orderer2.cloud-labs.io/tls/server.crt ServerTLSCert: ./artifacts/crypto-config/ordererOrganizations/cloud-labs.io/orderers/orderer2.cloud-labs.io/tls/server.crt - Host: orderer3.cloud-labs.io Port: 7050 ClientTLSCert: ./artifacts/crypto-config/ordererOrganizations/cloud-labs.io/orderers/orderer3.cloud-labs.io/tls/server.crt ServerTLSCert: ./artifacts/crypto-config/ordererOrganizations/cloud-labs.io/orderers/orderer3.cloud-labs.io/tls/server.crt - Host: orderer4.cloud-labs.io Port: 7050 ClientTLSCert: ./artifacts/crypto-config/ordererOrganizations/cloud-labs.io/orderers/orderer4.cloud-labs.io/tls/server.crt ServerTLSCert: ./artifacts/crypto-config/ordererOrganizations/cloud-labs.io/orderers/orderer4.cloud-labs.io/tls/server.crt Addresses: - orderer0.cloud-labs.io:7050 - orderer1.cloud-labs.io:7050 - orderer2.cloud-labs.io:7050 - orderer3.cloud-labs.io:7050 - orderer4.cloud-labs.io:7050 BatchTimeout: 2s BatchSize: MaxMessageCount: 10 AbsoluteMaxBytes: 99 MB PreferredMaxBytes: 512 KB Organizations: - *cloudLabs Policies: Readers: Type: ImplicitMeta Rule: \u0026#34;ANY Readers\u0026#34; Writers: Type: ImplicitMeta Rule: \u0026#34;ANY Writers\u0026#34; Admins: Type: ImplicitMeta Rule: \u0026#34;MAJORITY Admins\u0026#34; BlockValidation: Type: ImplicitMeta Rule: \u0026#34;ANY Writers\u0026#34; Channel: \u0026amp;ChannelDefaults Policies: # Who may invoke the \u0026#39;Deliver\u0026#39; API Readers: Type: ImplicitMeta Rule: \u0026#34;ANY Readers\u0026#34; # Who may invoke the \u0026#39;Broadcast\u0026#39; API Writers: Type: ImplicitMeta Rule: \u0026#34;ANY Writers\u0026#34; # By default, who may modify elements at this config level Admins: Type: ImplicitMeta Rule: \u0026#34;MAJORITY Admins\u0026#34; Capabilities: \u0026lt;\u0026lt;: *ChannelCapabilities Profiles: OrdererGenesis: \u0026lt;\u0026lt;: *ChannelDefaults Capabilities: \u0026lt;\u0026lt;: *ChannelCapabilities Orderer: \u0026lt;\u0026lt;: *OrdererDefaults Capabilities: \u0026lt;\u0026lt;: *OrdererCapabilities Application: \u0026lt;\u0026lt;: *ApplicationDefaults Organizations: - \u0026lt;\u0026lt;: *cloudLabs Consortiums: SeriesConsortium: Organizations: - *storage - *vhost SeriesChannel: Consortium: SeriesConsortium \u0026lt;\u0026lt;: *ChannelDefaults Application: \u0026lt;\u0026lt;: *ApplicationDefaults Organizations: - *storage - *vhost Capabilities: \u0026lt;\u0026lt;: *ApplicationCapabilities 生成证书和创世区块 1 2 3 4 5 6 7 8 9 10 11 12 13 #!/usr/bin/env bash clear FABRIC_TAG=2.0 echo \u0026#34;Generate crypto material\u0026#34; docker run -it --rm \\ -v \u0026#34;$PWD\u0026#34;:/hyperledger-fabric-network \\ -w /hyperledger-fabric-network \\ -e \u0026#34;FABRIC_CFG_PATH=/hyperledger-fabric-network\u0026#34; \\ hyperledger/fabric-tools:$FABRIC_TAG \\ sh -c \u0026#34;rm -rf ./artifacts/crypto-config/* \u0026amp;\u0026amp; cryptogen generate --config=./crypto-config.yaml --output=./artifacts/crypto-config\u0026#34; 1 2 3 4 5 6 7 8 9 10 11 12 13 #!/usr/bin/env bash clear FABRIC_TAG=2.0 PROFILE=OrdererGenesis echo \u0026#34;Generate genesis block for orderer\u0026#34; docker run -it --rm \\ -v \u0026#34;$PWD\u0026#34;:/hyperledger-fabric-network \\ -w /hyperledger-fabric-network \\ -e \u0026#34;FABRIC_CFG_PATH=/hyperledger-fabric-network\u0026#34; \\ hyperledger/fabric-tools:$FABRIC_TAG \\ sh -c \u0026#34;rm -f ./artifacts/config/genesis.block \u0026amp;\u0026amp; configtxgen -profile $PROFILE -outputBlock ./artifacts/config/genesis.block -channelID syschain\u0026#34; 生成Channel交易配置 1 2 3 4 5 6 7 8 9 10 11 12 13 14 #!/usr/bin/env bash clear FABRIC_TAG=2.0 PROFILE=SeriesChannel CHANNEL=alice echo \u0026#34;Generate channel configuration transaction\u0026#34; docker run -it --rm \\ -v \u0026#34;$PWD\u0026#34;:/hyperledger-fabric-network \\ -w /hyperledger-fabric-network \\ -e \u0026#34;FABRIC_CFG_PATH=/hyperledger-fabric-network\u0026#34; \\ hyperledger/fabric-tools:$FABRIC_TAG \\ sh -c \u0026#34;rm -f ./artifacts/config/$CHANNEL.tx \u0026amp;\u0026amp; configtxgen -profile $PROFILE -outputCreateChannelTx ./artifacts/config/$CHANNEL.tx -channelID $CHANNEL\u0026#34; 配置docker stack编排 docker-compose-storage.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 version: \u0026#39;3\u0026#39; networks: baas: external: name: baas services: ca0: image: hyperledger/fabric-ca:2.0 hostname: ca0.storage.cloud-labs.io environment: - FABRIC_CA_HOME=/etc/hyperledger/fabric-ca-server - FABRIC_CA_SERVER_CA_NAME=ca0.storage.cloud-labs.io - FABRIC_CA_SERVER_CA_CERTFILE=/etc/hyperledger/fabric-ca-server-config/ca/ca.storage.cloud-labs.io-cert.pem - FABRIC_CA_SERVER_CA_KEYFILE=/etc/hyperledger/fabric-ca-server-config/ca/380edf74737d53b1cc324f1d6c06f8b35bdcbe89799dbdbf146d95da681daec2_sk - FABRIC_CA_SERVER_TLS_ENABLED=true - FABRIC_CA_SERVER_TLS_CERTFILE=/etc/hyperledger/fabric-ca-server-config/tlsca/tlsca.storage.cloud-labs.io-cert.pem - FABRIC_CA_SERVER_TLS_KEYFILE=/etc/hyperledger/fabric-ca-server-config/tlsca/85d708dacaeb88897f6a3744e7cf84a53591049ab692c0ecd9c49a449a4050bf_sk - FABRIC_CA_SERVER_PORT=7054 command: sh -c \u0026#39;fabric-ca-server start -b admin:adminpw -d\u0026#39; ports: - 7054 networks: baas: aliases: - ca0.storage.cloud-labs.io volumes: - $PWD/artifacts/crypto-config/peerOrganizations/storage.cloud-labs.io/ca/:/etc/hyperledger/fabric-ca-server-config/ca/ - $PWD/artifacts/crypto-config/peerOrganizations/storage.cloud-labs.io/tlsca/:/etc/hyperledger/fabric-ca-server-config/tlsca/ peer0: image: hyperledger/fabric-peer:2.0 hostname: peer0.storage.cloud-labs.io environment: - FABRIC_LOGGING_SPEC=INFO - CORE_VM_ENDPOINT=unix:///host/var/run/docker.sock - CORE_VM_DOCKER_HOSTCONFIG_NETWORKMODE=baas - CORE_PEER_ID=peer0.storage.cloud-labs.io:7051 - CORE_PEER_LOCALMSPID=storageMSP - CORE_PEER_ADDRESS=peer0.storage.cloud-labs.io:7051 - CORE_PEER_LISTENADDRESS:0.0.0.0:7051 - CORE_PEER_GOSSIP_USELEADERELECTION=true - CORE_PEER_GOSSIP_ORGLEADER=false - CORE_PEER_GOSSIP_EXTERNALENDPOINT=peer0.storage.cloud-labs.io:7051 - CORE_PEER_PROFILE_ENABLED=true - CORE_CHAINCODE_EXECUTETIMEOUT=300s - CORE_PEER_CHAINCODEADDRESS=peer0.storage.cloud-labs.io:7052 - CORE_PEER_CHAINCODELISTENADDRESS=0.0.0.0:7052 - CORE_PEER_TLS_ENABLED=true - CORE_PEER_TLS_ROOTCERT_FILE=/etc/hyperledger/fabric/tls/ca.crt - CORE_PEER_TLS_CERT_FILE=/etc/hyperledger/fabric/tls/server.crt - CORE_PEER_TLS_KEY_FILE=/etc/hyperledger/fabric/tls/server.key command: peer node start networks: baas: aliases: - peer0.storage.cloud-labs.io ports: - 7051 - 7052 - 7053 volumes: - /var/run/:/host/var/run/ - $PWD/artifacts/crypto-config/peerOrganizations/storage.cloud-labs.io/peers/peer0.storage.cloud-labs.io/msp:/etc/hyperledger/fabric/msp - $PWD/artifacts/crypto-config/peerOrganizations/storage.cloud-labs.io/peers/peer0.storage.cloud-labs.io/tls:/etc/hyperledger/fabric/tls cli: image: hyperledger/fabric-tools:2.0 tty: true stdin_open: true hostname: cli.storage.cloud-labs.io environment: - GOPATH=/opt/gopath - FABRIC_LOGGING_SPEC=INFO - CORE_VM_ENDPOINT=unix:///host/var/run/docker.sock - CORE_VM_DOCKER_HOSTCONFIG_NETWORKMODE=baas - CORE_PEER_ID=peer0.storage.cloud-labs.io:7051 - CORE_PEER_LOCALMSPID=storageMSP - CORE_PEER_ADDRESS=peer0.storage.cloud-labs.io:7051 - CORE_PEER_MSPCONFIGPATH=/opt/gopath/src/github.com/hyperledger/fabric/peer/crypto/peerOrganizations/storage.cloud-labs.io/users/Admin@storage.cloud-labs.io/msp - CORE_PEER_TLS_ENABLED=true - CORE_PEER_TLS_CERT_FILE=/opt/gopath/src/github.com/hyperledger/fabric/peer/crypto/peerOrganizations/storage.cloud-labs.io/peers/peer0.storage.cloud-labs.io/tls/server.crt - CORE_PEER_TLS_KEY_FILE=/opt/gopath/src/github.com/hyperledger/fabric/peer/crypto/peerOrganizations/storage.cloud-labs.io/peers/peer0.storage.cloud-labs.io/tls/server.key - CORE_PEER_TLS_ROOTCERT_FILE=/opt/gopath/src/github.com/hyperledger/fabric/peer/crypto/peerOrganizations/storage.cloud-labs.io/peers/peer0.storage.cloud-labs.io/tls/ca.crt - ORDERER_SYSCHAN_ID=syschain - ORDERER_CA=/opt/gopath/src/github.com/hyperledger/fabric/orderer/crypto/ordererOrganizations/cloud-labs.io/orderers/orderer0.cloud-labs.io/msp/tlscacerts/tlsca.cloud-labs.io-cert.pem working_dir: /opt/gopath/src/github.com/hyperledger/fabric/peer command: /bin/bash networks: - baas volumes: - /var/run/:/host/var/run/ - $PWD/artifacts/chaincode:/opt/gopath/src - $PWD/artifacts/crypto-config/peerOrganizations:/opt/gopath/src/github.com/hyperledger/fabric/peer/crypto/peerOrganizations - $PWD/artifacts/crypto-config/ordererOrganizations:/opt/gopath/src/github.com/hyperledger/fabric/orderer/crypto/ordererOrganizations - $PWD/artifacts/config:/opt/gopath/src/github.com/hyperledger/fabric/peer/config - $PWD/artifacts/scripts:/opt/gopath/src/github.com/hyperledger/fabric/peer/scripts docker-compose-vhost.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 version: \u0026#39;3\u0026#39; networks: baas: external: name: baas services: ca0: image: hyperledger/fabric-ca:2.0 hostname: ca0.vhost.cloud-labs.io environment: - FABRIC_CA_HOME=/etc/hyperledger/fabric-ca-server - FABRIC_CA_SERVER_CA_NAME=ca0.vhost.cloud-labs.io - FABRIC_CA_SERVER_CA_CERTFILE=/etc/hyperledger/fabric-ca-server-config/ca/ca.vhost.cloud-labs.io-cert.pem - FABRIC_CA_SERVER_CA_KEYFILE=/etc/hyperledger/fabric-ca-server-config/ca/683dd1cb65ff6a48cb1b1566b380c0b76b2d3d4ce5b79800e5e5a98019c01dce_sk - FABRIC_CA_SERVER_TLS_ENABLED=true - FABRIC_CA_SERVER_TLS_CERTFILE=/etc/hyperledger/fabric-ca-server-config/tlsca/tlsca.vhost.cloud-labs.io-cert.pem - FABRIC_CA_SERVER_TLS_KEYFILE=/etc/hyperledger/fabric-ca-server-config/tlsca/e886c3cec95ad716b823542c342a0514d72a289d05213636d81cbb9071a75bd1_sk - FABRIC_CA_SERVER_PORT=7054 command: sh -c \u0026#39;fabric-ca-server start -b admin:adminpw -d\u0026#39; ports: - 7054 networks: baas: aliases: - ca0.vhost.cloud-labs.io volumes: - $PWD/artifacts/crypto-config/peerOrganizations/vhost.cloud-labs.io/ca/:/etc/hyperledger/fabric-ca-server-config/ca/ - $PWD/artifacts/crypto-config/peerOrganizations/vhost.cloud-labs.io/tlsca/:/etc/hyperledger/fabric-ca-server-config/tlsca/ peer0: image: hyperledger/fabric-peer:2.0 hostname: peer0.vhost.cloud-labs.io environment: - FABRIC_LOGGING_SPEC=INFO - CORE_VM_ENDPOINT=unix:///host/var/run/docker.sock - CORE_VM_DOCKER_HOSTCONFIG_NETWORKMODE=baas - CORE_PEER_ID=peer0.vhost.cloud-labs.io:7051 - CORE_PEER_LOCALMSPID=vhostMSP - CORE_PEER_ADDRESS=peer0.vhost.cloud-labs.io:7051 - CORE_PEER_LISTENADDRESS:0.0.0.0:7051 - CORE_PEER_GOSSIP_USELEADERELECTION=true - CORE_PEER_GOSSIP_ORGLEADER=false - CORE_PEER_GOSSIP_EXTERNALENDPOINT=peer0.vhost.cloud-labs.io:7051 - CORE_PEER_PROFILE_ENABLED=true - CORE_CHAINCODE_EXECUTETIMEOUT=300s - CORE_PEER_CHAINCODEADDRESS=peer0.vhost.cloud-labs.io:7052 - CORE_PEER_CHAINCODELISTENADDRESS=0.0.0.0:7052 - CORE_PEER_TLS_ENABLED=true - CORE_PEER_TLS_ROOTCERT_FILE=/etc/hyperledger/fabric/tls/ca.crt - CORE_PEER_TLS_CERT_FILE=/etc/hyperledger/fabric/tls/server.crt - CORE_PEER_TLS_KEY_FILE=/etc/hyperledger/fabric/tls/server.key command: peer node start networks: baas: aliases: - peer0.vhost.cloud-labs.io ports: - 7051 - 7052 - 7053 volumes: - /var/run/:/host/var/run/ - $PWD/artifacts/crypto-config/peerOrganizations/vhost.cloud-labs.io/peers/peer0.vhost.cloud-labs.io/msp:/etc/hyperledger/fabric/msp - $PWD/artifacts/crypto-config/peerOrganizations/vhost.cloud-labs.io/peers/peer0.vhost.cloud-labs.io/tls:/etc/hyperledger/fabric/tls cli: image: hyperledger/fabric-tools:2.0 tty: true stdin_open: true hostname: cli.vhost.cloud-labs.io environment: - GOPATH=/opt/gopath - FABRIC_LOGGING_SPEC=INFO - CORE_VM_ENDPOINT=unix:///host/var/run/docker.sock - CORE_VM_DOCKER_HOSTCONFIG_NETWORKMODE=baas - CORE_PEER_ID=peer0.vhost.cloud-labs.io:7051 - CORE_PEER_LOCALMSPID=vhostMSP - CORE_PEER_ADDRESS=peer0.vhost.cloud-labs.io:7051 - CORE_PEER_MSPCONFIGPATH=/opt/gopath/src/github.com/hyperledger/fabric/peer/crypto/peerOrganizations/vhost.cloud-labs.io/users/Admin@vhost.cloud-labs.io/msp - CORE_PEER_TLS_ENABLED=true - CORE_PEER_TLS_CERT_FILE=/opt/gopath/src/github.com/hyperledger/fabric/peer/crypto/peerOrganizations/vhost.cloud-labs.io/peers/peer0.vhost.cloud-labs.io/tls/server.crt - CORE_PEER_TLS_KEY_FILE=/opt/gopath/src/github.com/hyperledger/fabric/peer/crypto/peerOrganizations/vhost.cloud-labs.io/peers/peer0.vhost.cloud-labs.io/tls/server.key - CORE_PEER_TLS_ROOTCERT_FILE=/opt/gopath/src/github.com/hyperledger/fabric/peer/crypto/peerOrganizations/vhost.cloud-labs.io/peers/peer0.vhost.cloud-labs.io/tls/ca.crt - ORDERER_SYSCHAN_ID=syschain - ORDERER_CA=/opt/gopath/src/github.com/hyperledger/fabric/orderer/crypto/ordererOrganizations/cloud-labs.io/orderers/orderer0.cloud-labs.io/msp/tlscacerts/tlsca.cloud-labs.io-cert.pem working_dir: /opt/gopath/src/github.com/hyperledger/fabric/peer command: /bin/bash networks: - baas volumes: - /var/run/:/host/var/run/ - $PWD/artifacts/chaincode:/opt/gopath/src - $PWD/artifacts/crypto-config/peerOrganizations:/opt/gopath/src/github.com/hyperledger/fabric/peer/crypto/peerOrganizations - $PWD/artifacts/crypto-config/ordererOrganizations:/opt/gopath/src/github.com/hyperledger/fabric/orderer/crypto/ordererOrganizations - $PWD/artifacts/config:/opt/gopath/src/github.com/hyperledger/fabric/peer/config - $PWD/artifacts/scripts:/opt/gopath/src/github.com/hyperledger/fabric/peer/scripts docker-compose-orderer.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 version: \u0026#39;3\u0026#39; networks: baas: external: name: baas services: orderer0: image: hyperledger/fabric-orderer:2.0 hostname: orderer0.cloud-labs.io environment: - FABRIC_LOGGING_SPEC=INFO - ORDERER_GENERAL_LISTENADDRESS=0.0.0.0 - ORDERER_GENERAL_GENESISMETHOD=file - ORDERER_GENERAL_GENESISFILE=/var/hyperledger/orderer/orderer.genesis.block - ORDERER_GENERAL_LOCALMSPID=cloudLabsMSP - ORDERER_GENERAL_LOCALMSPDIR=/var/hyperledger/orderer/msp # enabled TLS - ORDERER_GENERAL_TLS_ENABLED=true - ORDERER_GENERAL_TLS_PRIVATEKEY=/var/hyperledger/orderer/tls/server.key - ORDERER_GENERAL_TLS_CERTIFICATE=/var/hyperledger/orderer/tls/server.crt - ORDERER_GENERAL_TLS_ROOTCAS=[/var/hyperledger/orderer/tls/ca.crt] - ORDERER_GENERAL_CLUSTER_CLIENTCERTIFICATE=/var/hyperledger/orderer/tls/server.crt - ORDERER_GENERAL_CLUSTER_CLIENTPRIVATEKEY=/var/hyperledger/orderer/tls/server.key - ORDERER_GENERAL_CLUSTER_ROOTCAS=[/var/hyperledger/orderer/tls/ca.crt] working_dir: /opt/gopath/src/github.com/hyperledger/fabric command: orderer networks: baas: aliases: - orderer0.cloud-labs.io ports: - 7050 volumes: - $PWD/artifacts/crypto-config/ordererOrganizations/cloud-labs.io/users:/var/hyperledger/users - $PWD/artifacts/config/genesis.block:/var/hyperledger/orderer/orderer.genesis.block - $PWD/artifacts/crypto-config/ordererOrganizations/cloud-labs.io/orderers/orderer0.cloud-labs.io/msp:/var/hyperledger/orderer/msp - $PWD/artifacts/crypto-config/ordererOrganizations/cloud-labs.io/orderers/orderer0.cloud-labs.io/tls/:/var/hyperledger/orderer/tls orderer1: image: hyperledger/fabric-orderer:2.0 hostname: orderer1.cloud-labs.io environment: - FABRIC_LOGGING_SPEC=INFO - ORDERER_GENERAL_LISTENADDRESS=0.0.0.0 - ORDERER_GENERAL_GENESISMETHOD=file - ORDERER_GENERAL_GENESISFILE=/var/hyperledger/orderer/orderer.genesis.block - ORDERER_GENERAL_LOCALMSPID=cloudLabsMSP - ORDERER_GENERAL_LOCALMSPDIR=/var/hyperledger/orderer/msp # enabled TLS - ORDERER_GENERAL_TLS_ENABLED=true - ORDERER_GENERAL_TLS_PRIVATEKEY=/var/hyperledger/orderer/tls/server.key - ORDERER_GENERAL_TLS_CERTIFICATE=/var/hyperledger/orderer/tls/server.crt - ORDERER_GENERAL_TLS_ROOTCAS=[/var/hyperledger/orderer/tls/ca.crt] - ORDERER_GENERAL_CLUSTER_CLIENTCERTIFICATE=/var/hyperledger/orderer/tls/server.crt - ORDERER_GENERAL_CLUSTER_CLIENTPRIVATEKEY=/var/hyperledger/orderer/tls/server.key - ORDERER_GENERAL_CLUSTER_ROOTCAS=[/var/hyperledger/orderer/tls/ca.crt] working_dir: /opt/gopath/src/github.com/hyperledger/fabric command: orderer networks: baas: aliases: - orderer1.cloud-labs.io ports: - 7050 volumes: - $PWD/artifacts/config/genesis.block:/var/hyperledger/orderer/orderer.genesis.block - $PWD/artifacts/crypto-config/ordererOrganizations/cloud-labs.io/orderers/orderer1.cloud-labs.io/msp:/var/hyperledger/orderer/msp - $PWD/artifacts/crypto-config/ordererOrganizations/cloud-labs.io/orderers/orderer1.cloud-labs.io/tls/:/var/hyperledger/orderer/tls orderer2: image: hyperledger/fabric-orderer:2.0 hostname: orderer2.cloud-labs.io environment: - FABRIC_LOGGING_SPEC=INFO - ORDERER_GENERAL_LISTENADDRESS=0.0.0.0 - ORDERER_GENERAL_GENESISMETHOD=file - ORDERER_GENERAL_GENESISFILE=/var/hyperledger/orderer/orderer.genesis.block - ORDERER_GENERAL_LOCALMSPID=cloudLabsMSP - ORDERER_GENERAL_LOCALMSPDIR=/var/hyperledger/orderer/msp # enabled TLS - ORDERER_GENERAL_TLS_ENABLED=true - ORDERER_GENERAL_TLS_PRIVATEKEY=/var/hyperledger/orderer/tls/server.key - ORDERER_GENERAL_TLS_CERTIFICATE=/var/hyperledger/orderer/tls/server.crt - ORDERER_GENERAL_TLS_ROOTCAS=[/var/hyperledger/orderer/tls/ca.crt] - ORDERER_GENERAL_CLUSTER_CLIENTCERTIFICATE=/var/hyperledger/orderer/tls/server.crt - ORDERER_GENERAL_CLUSTER_CLIENTPRIVATEKEY=/var/hyperledger/orderer/tls/server.key - ORDERER_GENERAL_CLUSTER_ROOTCAS=[/var/hyperledger/orderer/tls/ca.crt] working_dir: /opt/gopath/src/github.com/hyperledger/fabric command: orderer networks: baas: aliases: - orderer2.cloud-labs.io ports: - 7050 volumes: - $PWD/artifacts/config/genesis.block:/var/hyperledger/orderer/orderer.genesis.block - $PWD/artifacts/crypto-config/ordererOrganizations/cloud-labs.io/orderers/orderer2.cloud-labs.io/msp:/var/hyperledger/orderer/msp - $PWD/artifacts/crypto-config/ordererOrganizations/cloud-labs.io/orderers/orderer2.cloud-labs.io/tls/:/var/hyperledger/orderer/tls orderer3: image: hyperledger/fabric-orderer:2.0 hostname: orderer3.cloud-labs.io environment: - FABRIC_LOGGING_SPEC=INFO - ORDERER_GENERAL_LISTENADDRESS=0.0.0.0 - ORDERER_GENERAL_GENESISMETHOD=file - ORDERER_GENERAL_GENESISFILE=/var/hyperledger/orderer/orderer.genesis.block - ORDERER_GENERAL_LOCALMSPID=cloudLabsMSP - ORDERER_GENERAL_LOCALMSPDIR=/var/hyperledger/orderer/msp # enabled TLS - ORDERER_GENERAL_TLS_ENABLED=true - ORDERER_GENERAL_TLS_PRIVATEKEY=/var/hyperledger/orderer/tls/server.key - ORDERER_GENERAL_TLS_CERTIFICATE=/var/hyperledger/orderer/tls/server.crt - ORDERER_GENERAL_TLS_ROOTCAS=[/var/hyperledger/orderer/tls/ca.crt] - ORDERER_GENERAL_CLUSTER_CLIENTCERTIFICATE=/var/hyperledger/orderer/tls/server.crt - ORDERER_GENERAL_CLUSTER_CLIENTPRIVATEKEY=/var/hyperledger/orderer/tls/server.key - ORDERER_GENERAL_CLUSTER_ROOTCAS=[/var/hyperledger/orderer/tls/ca.crt] working_dir: /opt/gopath/src/github.com/hyperledger/fabric command: orderer networks: baas: aliases: - orderer3.cloud-labs.io ports: - 7050 volumes: - $PWD/artifacts/config/genesis.block:/var/hyperledger/orderer/orderer.genesis.block - $PWD/artifacts/crypto-config/ordererOrganizations/cloud-labs.io/orderers/orderer3.cloud-labs.io/msp:/var/hyperledger/orderer/msp - $PWD/artifacts/crypto-config/ordererOrganizations/cloud-labs.io/orderers/orderer3.cloud-labs.io/tls/:/var/hyperledger/orderer/tls orderer4: image: hyperledger/fabric-orderer:2.0 hostname: orderer4.cloud-labs.io environment: - FABRIC_LOGGING_SPEC=INFO - ORDERER_GENERAL_LISTENADDRESS=0.0.0.0 - ORDERER_GENERAL_GENESISMETHOD=file - ORDERER_GENERAL_GENESISFILE=/var/hyperledger/orderer/orderer.genesis.block - ORDERER_GENERAL_LOCALMSPID=cloudLabsMSP - ORDERER_GENERAL_LOCALMSPDIR=/var/hyperledger/orderer/msp # enabled TLS - ORDERER_GENERAL_TLS_ENABLED=true - ORDERER_GENERAL_TLS_PRIVATEKEY=/var/hyperledger/orderer/tls/server.key - ORDERER_GENERAL_TLS_CERTIFICATE=/var/hyperledger/orderer/tls/server.crt - ORDERER_GENERAL_TLS_ROOTCAS=[/var/hyperledger/orderer/tls/ca.crt] - ORDERER_GENERAL_CLUSTER_CLIENTCERTIFICATE=/var/hyperledger/orderer/tls/server.crt - ORDERER_GENERAL_CLUSTER_CLIENTPRIVATEKEY=/var/hyperledger/orderer/tls/server.key - ORDERER_GENERAL_CLUSTER_ROOTCAS=[/var/hyperledger/orderer/tls/ca.crt] working_dir: /opt/gopath/src/github.com/hyperledger/fabric command: orderer networks: baas: aliases: - orderer4.cloud-labs.io ports: - 7050 volumes: - $PWD/artifacts/config/genesis.block:/var/hyperledger/orderer/orderer.genesis.block - $PWD/artifacts/crypto-config/ordererOrganizations/cloud-labs.io/orderers/orderer4.cloud-labs.io/msp:/var/hyperledger/orderer/msp - $PWD/artifacts/crypto-config/ordererOrganizations/cloud-labs.io/orderers/orderer4.cloud-labs.io/tls/:/var/hyperledger/orderer/tls 部署 1 2 3 4 5 6 7 8 9 10 #!/usr/bin/env bash docker stack deploy -c docker-compose-storage.yaml storage_cloud-labs_io sleep 1 docker stack deploy -c docker-compose-vhost.yaml vhost_cloud-labs_io sleep 1 docker stack deploy -c docker-compose-orderer.yaml cloud-labs_io sleep 2 docker stack ls 创建Channel并且安装chaincode 1 2 3 4 5 6 7 8 #!/usr/bin/env bash clear CLI_NAME=storage_cloud-labs_io_cli ORDERER=orderer0.cloud-labs.io:7050 CHANNEL=alice docker exec -it -e \u0026#34;ORDERER=$ORDERER\u0026#34; -e \u0026#34;CHANNEL=$CHANNEL\u0026#34; \u0026#34;$(docker ps -q -f status=running --filter name=$CLI_NAME)\u0026#34; sh -c \u0026#39;chmod +x ./scripts/channel.tls.sh \u0026amp;\u0026amp; ./scripts/channel.tls.sh create\u0026#39; 1 2 3 4 5 6 7 8 #!/usr/bin/env bash clear CLI_NAMES=(storage_cloud-labs_io_cli vhost_cloud-labs_io_cli) ORDERER=orderer0.cloud-labs.io:7050 CHANNEL=alice for cli in \u0026#34;${CLI_NAMES[@]}\u0026#34;; do docker exec -it -e \u0026#34;ORDERER=$ORDERER\u0026#34; -e \u0026#34;CHANNEL=$CHANNEL\u0026#34; \u0026#34;$(docker ps -q -f status=running --filter name=\u0026#34;$cli\u0026#34;)\u0026#34; sh -c \u0026#39;chmod +x ./scripts/channel.tls.sh \u0026amp;\u0026amp; ./scripts/channel.tls.sh join\u0026#39; done 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 #!/usr/bin/env bash clear CLI_NAMES=(storage_cloud-labs_io_cli vhost_cloud-labs_io_cli) ORDERER=orderer0.cloud-labs.io:7050 CHANNEL=alice CC_NAME=c11 CC_VERSION=1.0 CC_PATH=github.com/c11 for cli in \u0026#34;${CLI_NAMES[@]}\u0026#34;; do CID=$(docker ps -q -f status=running --filter name=\u0026#34;$cli\u0026#34;) docker exec -it \\ -e \u0026#34;ORDERER=$ORDERER\u0026#34; \\ -e \u0026#34;CHANNEL=$CHANNEL\u0026#34; \\ -e \u0026#34;CC_NAME=$CC_NAME\u0026#34; \\ -e \u0026#34;CC_VERSION=$CC_VERSION\u0026#34; \\ -e \u0026#34;CC_PATH=$CC_PATH\u0026#34; \\ \u0026#34;$CID\u0026#34; sh -c \u0026#39;chmod +x ./scripts/chaincode.tls.sh \u0026amp;\u0026amp; ./scripts/chaincode.tls.sh install \u0026amp;\u0026amp; ./scripts/chaincode.tls.sh install_list\u0026#39; done 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 #!/usr/bin/env bash clear CLI_NAMES=(storage_cloud-labs_io_cli vhost_cloud-labs_io_cli) ORDERER=orderer0.cloud-labs.io:7050 CHANNEL=alice CC_NAME=c11 CC_VERSION=1.0 CC_CTR=\u0026#39;{\u0026#34;Args\u0026#34;:[\u0026#34;init\u0026#34;,\u0026#34;a\u0026#34;,\u0026#34;100\u0026#34;,\u0026#34;b\u0026#34;,\u0026#34;200\u0026#34;]}\u0026#39; for cli in \u0026#34;${CLI_NAMES[@]}\u0026#34;; do CID=$(docker ps -q -f status=running --filter name=\u0026#34;$cli\u0026#34;) docker exec -it \\ -e \u0026#34;ORDERER=$ORDERER\u0026#34; \\ -e \u0026#34;CHANNEL=$CHANNEL\u0026#34; \\ -e \u0026#34;CC_NAME=$CC_NAME\u0026#34; \\ -e \u0026#34;CC_VERSION=$CC_VERSION\u0026#34; \\ -e \u0026#34;CC_CTR=$CC_CTR\u0026#34; \\ \u0026#34;$CID\u0026#34; sh -c \u0026#39;chmod +x ./scripts/chaincode.tls.sh \u0026amp;\u0026amp; ./scripts/chaincode.tls.sh instantiate \u0026amp;\u0026amp; ./scripts/chaincode.tls.sh instantiate_list\u0026#39; done 测试 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 #!/usr/bin/env bash clear CLI_NAME=storage_cloud-labs_io_cli ORDERER=orderer0.cloud-labs.io:7050 CHANNEL=alice CC_NAME=c11 CC_QUERY_ARG=\u0026#39;{\u0026#34;Args\u0026#34;:[\u0026#34;query\u0026#34;,\u0026#34;a\u0026#34;]}\u0026#39; CID=$(docker ps -q -f status=running --filter name=\u0026#34;$CLI_NAME\u0026#34;) docker exec -it \\ -e \u0026#34;ORDERER=$ORDERER\u0026#34; \\ -e \u0026#34;CHANNEL=$CHANNEL\u0026#34; \\ -e \u0026#34;CC_NAME=$CC_NAME\u0026#34; \\ -e \u0026#34;CC_QUERY_ARG=$CC_QUERY_ARG\u0026#34; \\ \u0026#34;$CID\u0026#34; sh -c \u0026#39;chmod +x ./scripts/chaincode.tls.sh \u0026amp;\u0026amp; ./scripts/chaincode.tls.sh query\u0026#39; 1 2 3 4 5 6 7 8 9 10 11 12 13 14 #!/usr/bin/env bash CLI_NAME=storage_cloud-labs_io_cli ORDERER=orderer0.cloud-labs.io:7050 CHANNEL=alice CC_NAME=c11 CC_INVOKE_ARG=\u0026#39;{\u0026#34;Args\u0026#34;:[\u0026#34;invoke\u0026#34;,\u0026#34;a\u0026#34;,\u0026#34;b\u0026#34;,\u0026#34;10\u0026#34;]}\u0026#39; CID=$(docker ps -q -f status=running --filter name=\u0026#34;$CLI_NAME\u0026#34;) docker exec -it \\ -e \u0026#34;ORDERER=$ORDERER\u0026#34; \\ -e \u0026#34;CHANNEL=$CHANNEL\u0026#34; \\ -e \u0026#34;CC_NAME=$CC_NAME\u0026#34; \\ -e \u0026#34;CC_INVOKE_ARG=$CC_INVOKE_ARG\u0026#34; \\ \u0026#34;$CID\u0026#34; sh -c \u0026#39;chmod +x ./scripts/chaincode.tls.sh \u0026amp;\u0026amp; ./scripts/chaincode.tls.sh invoke\u0026#39; 总结 通过本次的交流，再次对Hyperledger Fabric进行了下学习，并且对1.4加入的Raft共识有了一次实践。\n本次的交流实践，我把它整理了放到我的仓库，感兴趣的可以去了解下！ 传送门\n","date":"2019-11-20T13:55:59+08:00","image":"https://ronggle.com/2019/hyperledger-fabric-on-swarm/hyperledger-fabric-on-swarm.svg","permalink":"https://ronggle.com/2019/hyperledger-fabric-on-swarm/","title":"使用Docker Swarm部署Hyperledger Fabric"},{"content":"继续挖坑，希望将来能静下心来，安静的补坑。\nLile是Go中的gRPC服务和一组工具/库的应用程序生成器（类似于create-react-app，rails new或django startproject）。\nLile的主要焦点是在创建新服务时通过创建基本结构，测试示例，Dockerfile，Makefile等。\nLile带有基本的预设置，带有可插拔选项，适用于\u0026hellip;\u0026hellip;\nMetrics (例如Prometheus) Tracing (例如Zipkin) PubSub (例如Google PubSub) Service Discovery 安装 安装Lile非常简单，使用go get就可以安装lile的命令行工具，用于生成新的服务和所需的库。\n首先，您需要安装Google的Protocol Buffers。\n1 2 $ brew install protobuf $ go get -u github.com/lileio/lile/... 快速开始 使用lile new生成新服务。\n1 $ lile new lileio/users 指南 创建服务 服务定义 生成RPC方法 编写并运行测试用例 使用生成的命令行 添加你自己的命令行 创建服务 Lile附带一个“代码生成器”，可以快速生成新的Lile服务。\nLile遵循Go关于$GOPATH的约定（参见如何写Go），并且自动解析您的新服务的名称，以在正确的位置创建服务。\n如果您的Github用户名是lileio，并且您想创建一个新的服务为了发布消息到Slack，您可以使用如下命令：\n1 lile new lileio/slack 这将创建一个项目到$GOPATH/src/github.com/lileio/slack\n服务定义 Lile服务主要使用gRPC，因此使用protocol buffers作为接口定义语言（IDL），用于描述有效负载消息的服务接口和结构。 如果需要，可以使用其他替代品。\n我强烈建议您先阅读Google API设计文档，以获得有关RPC方法和消息的一般命名的好建议，以及如果需要，可以通过gRPC Gateway将其转换为REST/JSON。\n下面是一个简单的例子account_service：\n1 2 3 4 5 6 7 8 9 10 11 12 service AccountService { rpc List (ListAccountsRequest) returns (ListAccountsResponse) {} rpc GetById (GetByIdRequest) returns (Account) {} rpc GetByEmail (GetByEmailRequest) returns (Account) {} rpc AuthenticateByEmail (AuthenticateByEmailRequest) returns (Account) {} rpc GeneratePasswordToken (GeneratePasswordTokenRequest) returns (GeneratePasswordTokenResponse) {} rpc ResetPassword (ResetPasswordRequest) returns (Account) {} rpc ConfirmAccount (ConfirmAccountRequest) returns (Account) {} rpc Create (CreateAccountRequest) returns (Account) {} rpc Update (UpdateAccountRequest) returns (Account) {} rpc Delete (DeleteAccountRequest) returns (google.protobuf.Empty) {} } 生成RPC方法 默认情况下，Lile将创建一个RPC方法和一个简单的请求和响应消息。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 syntax = \u0026#34;proto3\u0026#34;; option go_package = \u0026#34;github.com/lileio/slack\u0026#34;; package slack; message Request { string id = 1; } message Response { string id = 1; } service Slack { rpc Read (Request) returns (Response) {} } 我们来修改一下使它能够提供真正的服务，并添加自己的方法。\n我们来创建一个Announce方法向Slack发布消息。\n我们假设Slack团队和身份验证已经由服务配置来处理，所以我们服务的用户只需要提供一个房间和他们的消息。 该服务将发送特殊的空响应，因为我们只需要知道是否发生错误，也不需要知道其他任何内容。\n现在我们的proto文件看起来像这样：\n1 2 3 4 5 6 7 8 9 10 11 12 13 syntax = \u0026#34;proto3\u0026#34;; option go_package = \u0026#34;github.com/lileio/slack\u0026#34;; import \u0026#34;google/protobuf/empty.proto\u0026#34;; package slack; message AnnounceRequest { string channel = 1; string msg = 2; } service Slack { rpc Announce (AnnounceRequest) returns (google.protobuf.Empty) {} } 现在我们运行protoc工具我们的文件，以及Lile生成器插件。\n1 protoc -I . slack.proto --lile-server_out=. --go_out=plugins=grpc:$GOPATH/src Lile提供了一个Makefile，每个项目都有一个已经配置的proto构建步骤。 所以我们可以运行它。\n1 make proto 我们可以看到，Lile将在server目录中为我们创建两个文件。\n1 2 3 4 $ make proto protoc -I . slack.proto --lile-server_out=. --go_out=plugins=grpc:$GOPATH/src 2017/07/12 15:44:01 [Creating] server/announce.go 2017/07/12 15:44:01 [Creating test] server/announce_test.go 我们来看看Lile为我们创建的announce.go文件。\n1 2 3 4 5 6 7 8 9 10 11 12 13 package server import ( \u0026#34;errors\u0026#34; \u0026#34;github.com/golang/protobuf/ptypes/empty\u0026#34; \u0026#34;github.com/lileio/slack\u0026#34; context \u0026#34;golang.org/x/net/context\u0026#34; ) func (s SlackServer) Announce(ctx context.Context, r *slack.AnnounceRequest) (*empty.Empty, error) { return nil, errors.New(\u0026#34;not yet implemented\u0026#34;) } 接下来我们实现这个生成的方法，让我们从测试开始吧！\n编写并运行测试用例 当您使用Lile生成RPC方法时，也会创建一个对应的测试文件。例如，给定我们的announce.go文件，Lile将在同一目录中创建announce_test.go\n看起来如下:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 package server import ( \u0026#34;testing\u0026#34; \u0026#34;github.com/lileio/slack\u0026#34; \u0026#34;github.com/stretchr/testify/assert\u0026#34; context \u0026#34;golang.org/x/net/context\u0026#34; ) func TestAnnounce(t *testing.T) { ctx := context.Background() req := \u0026amp;slack.AnnounceRequest{} res, err := cli.Announce(ctx, req) assert.Nil(t, err) assert.NotNil(t, res) } 您现在可以使用Makefile运行测试，并运行make test命令:\n1 2 3 4 5 6 7 8 9 10 11 $ make test === RUN TestAnnounce --- FAIL: TestAnnounce (0.00s) Error Trace: announce_test.go:16 Error: Expected nil, but got: \u0026amp;status.statusError{Code:2, Message:\u0026#34;not yet implemented\u0026#34;, Details:[]*any.Any(nil)} Error Trace: announce_test.go:17 Error: Expected value not to be nil. FAIL coverage: 100.0% of statements FAIL github.com/lileio/slack/server 0.011s make: *** [test] Error 2 我们的测试失败了，因为我们还没有实现我们的方法，在我们的方法中返回一个“未实现”的错误。\n让我们在announce.go中实现Announce方法，这里是一个使用nlopes的slack library的例子。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 package server import ( \u0026#34;os\u0026#34; \u0026#34;google.golang.org/grpc/codes\u0026#34; \u0026#34;google.golang.org/grpc/status\u0026#34; \u0026#34;github.com/golang/protobuf/ptypes/empty\u0026#34; \u0026#34;github.com/lileio/slack\u0026#34; sl \u0026#34;github.com/nlopes/slack\u0026#34; context \u0026#34;golang.org/x/net/context\u0026#34; ) var api = sl.New(os.Getenv(\u0026#34;SLACK_TOKEN\u0026#34;)) func (s SlackServer) Announce(ctx context.Context, r *slack.AnnounceRequest) (*empty.Empty, error) { _, _, err := api.PostMessage(r.Channel, r.Msg, sl.PostMessageParameters{}) if err != nil { return nil, status.Errorf(codes.Internal, err.Error()) } return \u0026amp;empty.Empty{}, nil } 我们再次修改我们的测试用力，然后再次运行我们的测试:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 package server import ( \u0026#34;testing\u0026#34; \u0026#34;github.com/lileio/slack\u0026#34; \u0026#34;github.com/stretchr/testify/assert\u0026#34; context \u0026#34;golang.org/x/net/context\u0026#34; ) func TestAnnounce(t *testing.T) { ctx := context.Background() req := \u0026amp;slack.AnnounceRequest{ Channel: \u0026#34;@alex\u0026#34;, Msg: \u0026#34;hellooo\u0026#34;, } res, err := cli.Announce(ctx, req) assert.Nil(t, err) assert.NotNil(t, res) } 现在如果我使用我的Slack令牌作为环境变量运行测试，我应该看到通过测试！\n1 2 3 4 5 6 7 8 9 10 11 $ alex@slack: SLACK_TOKEN=zbxkkausdkasugdk make test go test -v ./... -cover ? github.com/lileio/slack [no test files] === RUN TestAnnounce --- PASS: TestAnnounce (0.32s) PASS coverage: 75.0% of statements ok github.com/lileio/slack/server 0.331s coverage: 75.0% of statements ? github.com/lileio/slack/slack [no test files] ? github.com/lileio/slack/slack/cmd [no test files] ? github.com/lileio/slack/subscribers [no test files] 使用生成的命令行 生成服务时，Lile会基于cobra生成命令行程序。您可以使用自己的cmd扩展应用程序，也可以使用内置cmds来运行服务。\n运行不带任何参数的cmd行应用程序将打印生成的帮助。\n例如go run orders/main.go\n启动 运行up将同时运行RPC服务器和发布订阅的订阅者。\n添加你自己的命令行 要添加自己的cmd，您可以使用Cobra内置的生成器来生成更多的命令。\n1 2 $ cd orders $ cobra add import 您现在可以编辑生成的文件，以创建您的命令行，cobra会自动将命令行的名称添加到帮助中。\n","date":"2019-07-07T17:47:56+08:00","image":"https://ronggle.com/2019/lile-getting-and-started/lile_huc07f78e848ff3c0cfda834824d5a3104_5265_120x120_fill_box_smart1_3.png","permalink":"https://ronggle.com/2019/lile-getting-and-started/","title":"Lile简单介绍"},{"content":"Elassandra Elassandra是Elasticsearch的一个分支，经过修改，可以作为Apache Cassandra的插件运行，具有可扩展和灵活的点对点架构。 Elasticsearch代码嵌入在Cassanda节点中，在Cassandra表上提供高级搜索功能，Cassandra用作Elasticsearch数据和配置存储。\nElassandra支持Cassandra vnodes，并通过添加更多节点进行水平扩展。\n项目文档可在doc.elassandra.io上获得。\nElassandra的好处 对于Cassandra用户，elassandra提供Elasticsearch功能：\n在Elasticsearch中更新Cassandra索引。 对Cassandra数据进行全文和空间搜索。 实时聚合（不需要Spark或Hadoop来完成GROUP BY） 在一个查询中提供对多个键空间和表的搜索。 使用“用户定义的类型”提供自动模式创建和支持嵌套文档。 提供JSON REST API对Cassandra数据的读/写访问。 许多Elasticsearch插件和Kibana等产品。 管理并发弹性搜索映射更改并应用批处理原子CQL架构更改。 支持Elasticsearch摄取处理器，允许转换输入数据。 对于Elasticsearch用户，elassandra提供了有用的功能：\nElassandra是无主的。群集状态通过cassandra轻量级事务进行管理。 Elassandra是一个分片的多主数据库，其中Elasticsearch是分片主从。因此，Elassandra没有单点写入，有助于实现高可用性。 Elassandra继承了Cassandra数据修复机制（暗示切换，读取修复和nodetool修复），为跨数据中心复制提供支持。 将节点添加到Elassandra集群时，只有从现有节点提取的数据才会在Elasticsearch中重新编制索引。 Cassandra可能是您索引和非索引数据的唯一数据存储区。它更易于管理和保护。源文档现在存储在Cassandra中，如果您需要NoSQL数据库和Elasticsearch，则会减少磁盘空间。 写操作不限于一个主分片，而是分布在虚拟数据中心的所有Cassandra节点上。分片数量不会限制您的写入吞吐量。添加elassandra节点会增加读写吞吐量。 Elasticsearch索引可以在许多Cassandra数据中心之间复制，允许写入最近的数据中心并进行全局搜索。 cassandra驱动程序可识别数据中心和令牌，提供自动负载平衡和故障转移。 Elassandra有效地将Elasticsearch文档存储在二进制SSTable中，而不会产生任何JSON开销。 快速开始 使用docker启动单节点的Elassandra集群：\n1 2 3 4 5 6 7 8 9 # 下载docker镜像 $ docker pull docker.io/strapdata/elassandra:latest # 启动 $ docker run -d --rm \\ --name elassandra \\ -p 9042:9042 \\ -p 9200:9200 \\ -e JVM_OPTS=\u0026#34;-Dcassandra.custom_query_handler_class=org.elassandra.index.ElasticQueryHandler\u0026#34; \\ docker.io/strapdata/elassandra:latest 检查Elassandra集群状态：\n1 2 3 4 5 6 7 $ docker exec -i elassandra nodetool status Datacenter: DC1 =============== Status=Up/Down |/ State=Normal/Leaving/Joining/Moving -- Address Load Tokens Owns (effective) Host ID Rack UN 172.17.0.3 80.97 KiB 8 100.0% 81a9e4e0-efe4-458d-861e-8d527835372d r1 从Cassandra表创建Elasticsearch索引 使用cassandra CQLSH创建一个cassandra Keyspace，一个User Defined Type，一个Table并添加两行：\n1 2 3 4 5 6 7 $ docker exec -i elassandra cqlsh \u0026lt;\u0026lt;EOF CREATE KEYSPACE IF NOT EXISTS test WITH replication = {\u0026#39;class\u0026#39;: \u0026#39;NetworkTopologyStrategy\u0026#39;, \u0026#39;DC1\u0026#39;: 1}; CREATE TYPE IF NOT EXISTS test.user_type (first text, last text); CREATE TABLE IF NOT EXISTS test.docs (uid int, username frozen\u0026lt;user_type\u0026gt;, login text, PRIMARY KEY (uid)); INSERT INTO test.docs (uid, username, login) VALUES (1, {first:\u0026#39;vince\u0026#39;,last:\u0026#39;royer\u0026#39;}, \u0026#39;vroyer\u0026#39;); INSERT INTO test.docs (uid, username, login) VALUES (2, {first:\u0026#39;barthelemy\u0026#39;,last:\u0026#39;delemotte\u0026#39;}, \u0026#39;barth\u0026#39;); EOF 通过发现CQL结构从Cassandra表架构创建Elasticsearch索引：\n1 2 $ curl -XPUT -H \u0026#39;Content-Type: application/json\u0026#39; http://localhost:9200/test -d\u0026#39;{\u0026#34;mappings\u0026#34;:{\u0026#34;docs\u0026#34;:{\u0026#34;discover\u0026#34;:\u0026#34;.*\u0026#34;}}}\u0026#39; {\u0026#34;acknowledged\u0026#34;:true,\u0026#34;shards_acknowledged\u0026#34;:true,\u0026#34;index\u0026#34;:\u0026#34;test\u0026#34;} 此命令发现与提供的正则表达式匹配的所有列，并创建Eslasticsearch索引。\n从头开始创建Elasticsearch索引 Elassandra在创建索引或使用新字段更新映射时自动生成基础CQL结构。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 $ curl -XPUT -H \u0026#39;Content-Type: application/json\u0026#39; http://localhost:9200/test2 -d\u0026#39;{ \u0026#34;mappings\u0026#34;:{ \u0026#34;docs\u0026#34;:{ \u0026#34;properties\u0026#34;: { \u0026#34;first\u0026#34;: { \u0026#34;type\u0026#34;:\u0026#34;text\u0026#34; }, \u0026#34;last\u0026#34;: { \u0026#34;type\u0026#34;:\u0026#34;text\u0026#34;, \u0026#34;cql_collection\u0026#34;:\u0026#34;singleton\u0026#34; } } } } }\u0026#39; {\u0026#34;acknowledged\u0026#34;:true,\u0026#34;shards_acknowledged\u0026#34;:true,\u0026#34;index\u0026#34;:\u0026#34;test2\u0026#34;} 生成的CQL结构：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 $ docker exec -it elassandra cqlsh Connected to Test Cluster at 127.0.0.1:9042. [cqlsh 5.0.1 | Cassandra 3.11.4.2 | CQL spec 3.4.4 | Native protocol v4] Use HELP for help. cqlsh\u0026gt; desc KEYSPACE test2; CREATE KEYSPACE test2 WITH replication = {\u0026#39;class\u0026#39;: \u0026#39;NetworkTopologyStrategy\u0026#39;, \u0026#39;DC1\u0026#39;: \u0026#39;1\u0026#39;} AND durable_writes = true; CREATE TABLE test2.docs ( \u0026#34;_id\u0026#34; text PRIMARY KEY, first list\u0026lt;text\u0026gt;, last text ) WITH bloom_filter_fp_chance = 0.01 AND caching = {\u0026#39;keys\u0026#39;: \u0026#39;ALL\u0026#39;, \u0026#39;rows_per_partition\u0026#39;: \u0026#39;NONE\u0026#39;} AND comment = \u0026#39;\u0026#39; AND compaction = {\u0026#39;class\u0026#39;: \u0026#39;org.apache.cassandra.db.compaction.SizeTieredCompactionStrategy\u0026#39;, \u0026#39;max_threshold\u0026#39;: \u0026#39;32\u0026#39;, \u0026#39;min_threshold\u0026#39;: \u0026#39;4\u0026#39;} AND compression = {\u0026#39;chunk_length_in_kb\u0026#39;: \u0026#39;64\u0026#39;, \u0026#39;class\u0026#39;: \u0026#39;org.apache.cassandra.io.compress.LZ4Compressor\u0026#39;} AND crc_check_chance = 1.0 AND dclocal_read_repair_chance = 0.1 AND default_time_to_live = 0 AND gc_grace_seconds = 864000 AND max_index_interval = 2048 AND memtable_flush_period_in_ms = 0 AND min_index_interval = 128 AND read_repair_chance = 0.0 AND speculative_retry = \u0026#39;99PERCENTILE\u0026#39;; CREATE CUSTOM INDEX elastic_docs_idx ON test2.docs () USING \u0026#39;org.elassandra.index.ExtendedElasticSecondaryIndex\u0026#39;; 搜索文档 通过Elasticsearch API搜索文档：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 $ curl \u0026#34;http://localhost:9200/test/_search?pretty\u0026#34; { \u0026#34;took\u0026#34; : 53, \u0026#34;timed_out\u0026#34; : false, \u0026#34;_shards\u0026#34; : { \u0026#34;total\u0026#34; : 1, \u0026#34;successful\u0026#34; : 1, \u0026#34;skipped\u0026#34; : 0, \u0026#34;failed\u0026#34; : 0 }, \u0026#34;hits\u0026#34; : { \u0026#34;total\u0026#34; : 2, \u0026#34;max_score\u0026#34; : 1.0, \u0026#34;hits\u0026#34; : [ { \u0026#34;_index\u0026#34; : \u0026#34;test\u0026#34;, \u0026#34;_type\u0026#34; : \u0026#34;docs\u0026#34;, \u0026#34;_id\u0026#34; : \u0026#34;1\u0026#34;, \u0026#34;_score\u0026#34; : 1.0, \u0026#34;_source\u0026#34; : { \u0026#34;uid\u0026#34; : 1, \u0026#34;login\u0026#34; : \u0026#34;vroyer\u0026#34;, \u0026#34;username\u0026#34; : { \u0026#34;last\u0026#34; : \u0026#34;royer\u0026#34;, \u0026#34;first\u0026#34; : \u0026#34;vince\u0026#34; } } }, { \u0026#34;_index\u0026#34; : \u0026#34;test\u0026#34;, \u0026#34;_type\u0026#34; : \u0026#34;docs\u0026#34;, \u0026#34;_id\u0026#34; : \u0026#34;2\u0026#34;, \u0026#34;_score\u0026#34; : 1.0, \u0026#34;_source\u0026#34; : { \u0026#34;uid\u0026#34; : 2, \u0026#34;login\u0026#34; : \u0026#34;barth\u0026#34;, \u0026#34;username\u0026#34; : { \u0026#34;last\u0026#34; : \u0026#34;delemotte\u0026#34;, \u0026#34;first\u0026#34; : \u0026#34;barthelemy\u0026#34; } } } ] } } 要通过CQL驱动程序搜索文档，请在表模式中添加以下两个虚拟列。 然后，执行Elasticsearch嵌套查询。 伪列允许您在索引名称与键空间名称不匹配时指定目标索引。\n1 2 3 4 5 6 7 8 9 10 11 12 13 $ docker exec -it elassandra cqlsh Connected to Test Cluster at 127.0.0.1:9042. [cqlsh 5.0.1 | Cassandra 3.11.4.2 | CQL spec 3.4.4 | Native protocol v4] Use HELP for help. cqlsh\u0026gt; ALTER TABLE test.docs ADD es_query text; cqlsh\u0026gt; ALTER TABLE test.docs ADD es_options text; cqlsh\u0026gt; SELECT uid, login, username FROM test.docs WHERE es_query=\u0026#39;{ \u0026#34;query\u0026#34;:{\u0026#34;nested\u0026#34;:{\u0026#34;path\u0026#34;:\u0026#34;username\u0026#34;,\u0026#34;query\u0026#34;:{\u0026#34;term\u0026#34;:{\u0026#34;username.first\u0026#34;:\u0026#34;barthelemy\u0026#34;}}}}}\u0026#39; AND es_options=\u0026#39;indices=test\u0026#39; ALLOW FILTERING; uid | login | username -----+-------+------------------------------------------ 2 | barth | {first: \u0026#39;barthelemy\u0026#39;, last: \u0026#39;delemotte\u0026#39;} (1 rows) 管理Elasticsearch索引 获取Elasticsearch集群状态：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 $ curl \u0026#34;http://localhost:9200/_cluster/state?pretty\u0026#34; { \u0026#34;cluster_name\u0026#34; : \u0026#34;Test Cluster\u0026#34;, \u0026#34;compressed_size_in_bytes\u0026#34; : 730, \u0026#34;version\u0026#34; : 14, \u0026#34;state_uuid\u0026#34; : \u0026#34;tMqNt8PpS5ySuaRxHUGc1w\u0026#34;, \u0026#34;master_node\u0026#34; : \u0026#34;81a9e4e0-efe4-458d-861e-8d527835372d\u0026#34;, \u0026#34;blocks\u0026#34; : { }, \u0026#34;nodes\u0026#34; : { \u0026#34;81a9e4e0-efe4-458d-861e-8d527835372d\u0026#34; : { \u0026#34;name\u0026#34; : \u0026#34;172.17.0.3\u0026#34;, \u0026#34;status\u0026#34; : \u0026#34;ALIVE\u0026#34;, \u0026#34;ephemeral_id\u0026#34; : \u0026#34;81a9e4e0-efe4-458d-861e-8d527835372d\u0026#34;, \u0026#34;transport_address\u0026#34; : \u0026#34;172.17.0.3:9300\u0026#34;, \u0026#34;attributes\u0026#34; : { \u0026#34;rack\u0026#34; : \u0026#34;r1\u0026#34;, \u0026#34;dc\u0026#34; : \u0026#34;DC1\u0026#34; } } }, \u0026#34;metadata\u0026#34; : { \u0026#34;version\u0026#34; : 4, \u0026#34;cluster_uuid\u0026#34; : \u0026#34;81a9e4e0-efe4-458d-861e-8d527835372d\u0026#34;, \u0026#34;templates\u0026#34; : { }, \u0026#34;indices\u0026#34; : { \u0026#34;test\u0026#34; : { \u0026#34;state\u0026#34; : \u0026#34;open\u0026#34;, \u0026#34;settings\u0026#34; : { \u0026#34;index\u0026#34; : { \u0026#34;creation_date\u0026#34; : \u0026#34;1561883488853\u0026#34;, \u0026#34;number_of_shards\u0026#34; : \u0026#34;1\u0026#34;, \u0026#34;number_of_replicas\u0026#34; : \u0026#34;0\u0026#34;, \u0026#34;uuid\u0026#34; : \u0026#34;h34je53mSVqojho2gIH91A\u0026#34;, \u0026#34;version\u0026#34; : { \u0026#34;created\u0026#34; : \u0026#34;6020399\u0026#34; }, \u0026#34;provided_name\u0026#34; : \u0026#34;test\u0026#34; } }, \u0026#34;mappings\u0026#34; : { \u0026#34;docs\u0026#34; : { \u0026#34;properties\u0026#34; : { \u0026#34;uid\u0026#34; : { \u0026#34;cql_partition_key\u0026#34; : true, \u0026#34;cql_primary_key_order\u0026#34; : 0, \u0026#34;type\u0026#34; : \u0026#34;integer\u0026#34;, \u0026#34;cql_collection\u0026#34; : \u0026#34;singleton\u0026#34; }, \u0026#34;login\u0026#34; : { \u0026#34;type\u0026#34; : \u0026#34;keyword\u0026#34;, \u0026#34;cql_collection\u0026#34; : \u0026#34;singleton\u0026#34; }, \u0026#34;username\u0026#34; : { \u0026#34;cql_udt_name\u0026#34; : \u0026#34;user_type\u0026#34;, \u0026#34;type\u0026#34; : \u0026#34;nested\u0026#34;, \u0026#34;properties\u0026#34; : { \u0026#34;last\u0026#34; : { \u0026#34;type\u0026#34; : \u0026#34;keyword\u0026#34;, \u0026#34;cql_collection\u0026#34; : \u0026#34;singleton\u0026#34; }, \u0026#34;first\u0026#34; : { \u0026#34;type\u0026#34; : \u0026#34;keyword\u0026#34;, \u0026#34;cql_collection\u0026#34; : \u0026#34;singleton\u0026#34; } }, \u0026#34;cql_collection\u0026#34; : \u0026#34;singleton\u0026#34; } } } }, \u0026#34;aliases\u0026#34; : [ ], \u0026#34;primary_terms\u0026#34; : { \u0026#34;0\u0026#34; : 0 }, \u0026#34;in_sync_allocations\u0026#34; : { \u0026#34;0\u0026#34; : [ ] } }, \u0026#34;test2\u0026#34; : { \u0026#34;state\u0026#34; : \u0026#34;open\u0026#34;, \u0026#34;settings\u0026#34; : { \u0026#34;index\u0026#34; : { \u0026#34;creation_date\u0026#34; : \u0026#34;1561883720880\u0026#34;, \u0026#34;number_of_shards\u0026#34; : \u0026#34;1\u0026#34;, \u0026#34;number_of_replicas\u0026#34; : \u0026#34;0\u0026#34;, \u0026#34;uuid\u0026#34; : \u0026#34;u7BgODwuT_idHbcuenvi6g\u0026#34;, \u0026#34;version\u0026#34; : { \u0026#34;created\u0026#34; : \u0026#34;6020399\u0026#34; }, \u0026#34;provided_name\u0026#34; : \u0026#34;test2\u0026#34; } }, \u0026#34;mappings\u0026#34; : { \u0026#34;docs\u0026#34; : { \u0026#34;properties\u0026#34; : { \u0026#34;last\u0026#34; : { \u0026#34;type\u0026#34; : \u0026#34;text\u0026#34;, \u0026#34;cql_collection\u0026#34; : \u0026#34;singleton\u0026#34; }, \u0026#34;first\u0026#34; : { \u0026#34;type\u0026#34; : \u0026#34;text\u0026#34; } } } }, \u0026#34;aliases\u0026#34; : [ ], \u0026#34;primary_terms\u0026#34; : { \u0026#34;0\u0026#34; : 0 }, \u0026#34;in_sync_allocations\u0026#34; : { \u0026#34;0\u0026#34; : [ ] } } }, \u0026#34;index-graveyard\u0026#34; : { \u0026#34;tombstones\u0026#34; : [ ] } }, \u0026#34;routing_table\u0026#34; : { \u0026#34;indices\u0026#34; : { \u0026#34;test\u0026#34; : { \u0026#34;shards\u0026#34; : { \u0026#34;0\u0026#34; : [ { \u0026#34;state\u0026#34; : \u0026#34;STARTED\u0026#34;, \u0026#34;primary\u0026#34; : true, \u0026#34;node\u0026#34; : \u0026#34;81a9e4e0-efe4-458d-861e-8d527835372d\u0026#34;, \u0026#34;relocating_node\u0026#34; : null, \u0026#34;shard\u0026#34; : 0, \u0026#34;index\u0026#34; : \u0026#34;test\u0026#34;, \u0026#34;token_ranges\u0026#34; : [ \u0026#34;(-9223372036854775808,9223372036854775807]\u0026#34; ], \u0026#34;allocation_id\u0026#34; : { \u0026#34;id\u0026#34; : \u0026#34;dummy_alloc_id\u0026#34; } } ] } }, \u0026#34;test2\u0026#34; : { \u0026#34;shards\u0026#34; : { \u0026#34;0\u0026#34; : [ { \u0026#34;state\u0026#34; : \u0026#34;STARTED\u0026#34;, \u0026#34;primary\u0026#34; : true, \u0026#34;node\u0026#34; : \u0026#34;81a9e4e0-efe4-458d-861e-8d527835372d\u0026#34;, \u0026#34;relocating_node\u0026#34; : null, \u0026#34;shard\u0026#34; : 0, \u0026#34;index\u0026#34; : \u0026#34;test2\u0026#34;, \u0026#34;token_ranges\u0026#34; : [ \u0026#34;(-9223372036854775808,9223372036854775807]\u0026#34; ], \u0026#34;allocation_id\u0026#34; : { \u0026#34;id\u0026#34; : \u0026#34;dummy_alloc_id\u0026#34; } } ] } } } }, \u0026#34;routing_nodes\u0026#34; : { \u0026#34;unassigned\u0026#34; : [ ], \u0026#34;nodes\u0026#34; : { \u0026#34;81a9e4e0-efe4-458d-861e-8d527835372d\u0026#34; : [ { \u0026#34;state\u0026#34; : \u0026#34;STARTED\u0026#34;, \u0026#34;primary\u0026#34; : true, \u0026#34;node\u0026#34; : \u0026#34;81a9e4e0-efe4-458d-861e-8d527835372d\u0026#34;, \u0026#34;relocating_node\u0026#34; : null, \u0026#34;shard\u0026#34; : 0, \u0026#34;index\u0026#34; : \u0026#34;test\u0026#34;, \u0026#34;token_ranges\u0026#34; : [ \u0026#34;(-9223372036854775808,9223372036854775807]\u0026#34; ], \u0026#34;allocation_id\u0026#34; : { \u0026#34;id\u0026#34; : \u0026#34;dummy_alloc_id\u0026#34; } }, { \u0026#34;state\u0026#34; : \u0026#34;STARTED\u0026#34;, \u0026#34;primary\u0026#34; : true, \u0026#34;node\u0026#34; : \u0026#34;81a9e4e0-efe4-458d-861e-8d527835372d\u0026#34;, \u0026#34;relocating_node\u0026#34; : null, \u0026#34;shard\u0026#34; : 0, \u0026#34;index\u0026#34; : \u0026#34;test2\u0026#34;, \u0026#34;token_ranges\u0026#34; : [ \u0026#34;(-9223372036854775808,9223372036854775807]\u0026#34; ], \u0026#34;allocation_id\u0026#34; : { \u0026#34;id\u0026#34; : \u0026#34;dummy_alloc_id\u0026#34; } } ] } }, \u0026#34;snapshots\u0026#34; : { \u0026#34;snapshots\u0026#34; : [ ] }, \u0026#34;restore\u0026#34; : { \u0026#34;snapshots\u0026#34; : [ ] }, \u0026#34;snapshot_deletions\u0026#34; : { \u0026#34;snapshot_deletions\u0026#34; : [ ] } } 获取Elasticsearch索引信息：\n1 2 3 4 $ curl \u0026#34;http://localhost:9200/_cat/indices?v\u0026#34; health status index uuid pri rep docs.count docs.deleted store.size pri.store.size green open test2 u7BgODwuT_idHbcuenvi6g 1 0 0 0 208b 208b green open test h34je53mSVqojho2gIH91A 1 0 4 0 4kb 4kb 删除Elasticsearch索引（默认情况下不删除底层的Cassandra表）：\n1 2 curl -XDELETE http://localhost:9200/test {\u0026#34;acknowledged\u0026#34;:true} ","date":"2019-03-08T11:27:29+08:00","image":"https://ronggle.com/2019/elassandra-getting-and-started/elassandra-logo_hua9e5fe0b2e07510695edbfcb16d06847_2125_120x120_fill_box_smart1_3.png","permalink":"https://ronggle.com/2019/elassandra-getting-and-started/","title":"Elassandra简单介绍"},{"content":"关系型数据库 关系数据库，是建立在关系模型基础上的数据库，借助于集合代数等数学概念和方法来处理数据库中的数据。\n现实世界中的各种实体以及实体之间的各种联系均用关系模型来表示。关系模型是由埃德加·科德于1970年首先提出的，并配合“科德十二定律”。现如今虽然对此模型有一些批评意见，但它还是数据存储的传统标准。标准数据查询语言SQL就是一种基于关系数据库的语言，这种语言执行对关系数据库中数据的检索和操作。 关系模型由关系数据结构、关系操作集合、关系完整性约束三部分组成。\n简单说，关系型数据库是由多张能互相联接的二维行列表格组成的数据库。\nNoSQL NoSQL(Not Only SQL)，泛指非关系型的数据库。\n随着互联网Web2.0网站的兴起，传统的关系数据库在应付Web2.0网站，特别是超大规模和高并发的SNS类型的Web2.0纯动态网站已经显得力不从心，暴露了很多难以克服的问题，而非关系型的数据库则由于其本身的特点得到了非常迅速的发展。NoSQL数据库的产生就是为了解决大规模数据集合多重数据种类带来的挑战，尤其是大数据应用难题。\nNoSQL数据库的四大分类:\nKV数据库 图型数据库 文档型数据库 列存储数据库 今天我们要说的，便是图形数据库。\n图形数据库 图形数据库是NoSQL数据库的一种类型，它应用图形理论存储实体之间的关系信息。图形数据库是一种非关系型数据库，它应用图形理论存储实体之间的关系信息。最常见例子就是社会网络中人与人之间的关系。关系型数据库用于存储“关系型”数据的效果并不好，其查询复杂、缓慢、超出预期，而图形数据库的独特设计恰恰弥补了这个缺陷。\n和其他以列、行或者KV等形式存储数据的数据库不同，图形数据库以节点（Node）和边（Edge）的网络存储所有信息。边表示那些代表对象的节点之间的联系。因为边和节点都可以被描述为对象，开发者可以为其指定属性（Attribute，或者 property）。为边增加方向最终会创建一个属性图，它代表图形数据库中的明确结构。\n我们为什么需要图形数据库？ 图形数据库可以通过使用操作、所有权和父项关系等来表示实体之间的关联关系。如果实体间的连接或关系是您正在尝试建模的数据的核心，那就适合使用图形数据库。因此，图形数据库对于建模和查询社交网络、推荐引擎、知识图谱、驾驶方向 (路线查找)、业务关系、依赖关系、货物移动等类似项目非常有用。\n社交网络 适合采用图形的常见使用案例的一个示例就是社交网络数据。 推荐引擎 可以在图形数据库中存储客户兴趣、好友和购买历史等信息类别之间的关系。然后，快速查询它以提出个性化和相关的建议。例如，可以使用高度可用的图形数据库，根据关注相同运动内容且具有类似购买历史记录的其他人购买的产品，向用户提供产品推荐。或者，可以识别有共同好友但还不认识对方的人员，然后提供好友推荐。\n知识图谱 利用知识图形，您可以将信息存储在图形模型中，并可以使用图形查询帮助用户更轻松地导航高度关联的数据集。例如，如果用户对 Leonardo da Vinci 创作的 Mona Lisa 感兴趣，您可以帮助他们发现同一艺术家的其他艺术作品或发现在卢浮宫展览的其他作品。 利用知识图谱，您可以将主题信息添加到产品目录，构建和查询复杂的监管规则模型，或者进行一般信息建模 (如维基数据)。\nNeo4j Neo4j是世界领先的图形数据库。 它是一个高性能的图形存储，具有成熟和强大的数据库所需的所有功能，如友好的查询语言和ACID事务。 程序员使用灵活的节点和关系网络结构而不是静态表 - 但享受企业级数据库的所有好处。 对于许多应用程序，与关系数据库相比，Neo4j提供了数量级的性能优势。\n安装 老样子，我依旧会适用Docker来进行安装。\n1 2 3 4 5 6 $ docker run -d \\ --name neo4j \\ -p 7474:7474 -p 7687:7687 \\ -v $HOME/neo4j/data:/data \\ -v $HOME/neo4j/logs:/logs \\ neo4j 实例 添加依赖 1 2 3 4 dependencies { compile(\u0026#34;org.neo4j:neo4j-ogm-core:3.1.7\u0026#34;) compile(\u0026#34;org.neo4j:neo4j-ogm-http-driver:3.1.7\u0026#34;) } 代码 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 package team.soi.demo; import lombok.Getter; import lombok.Setter; import org.neo4j.ogm.annotation.GeneratedValue; import org.neo4j.ogm.annotation.Id; import org.neo4j.ogm.annotation.NodeEntity; import org.neo4j.ogm.annotation.Relationship; import org.neo4j.ogm.config.Configuration; import org.neo4j.ogm.session.Session; import org.neo4j.ogm.session.SessionFactory; import java.io.Serializable; import java.util.ArrayList; import java.util.Arrays; import java.util.List; public class App { public static void main(String[] args) { Configuration configuration = new Configuration.Builder() .uri(\u0026#34;http://neo4j:qazplm@localhost:7474\u0026#34;) .build(); SessionFactory sessionFactory = new SessionFactory(configuration, \u0026#34;team.soi.demo\u0026#34;); Session session = sessionFactory.openSession(); User tom = new User(); tom.setName(\u0026#34;Tom\u0026#34;); User bobby = new User(); bobby.setName(\u0026#34;Bobby\u0026#34;); User linda = new User(); linda.setName(\u0026#34;Linda\u0026#34;); linda.setFriends(Arrays.asList(tom, bobby)); session.save(linda); } } @NodeEntity @Getter @Setter class User implements Serializable { @Id @GeneratedValue private Long id; private String name; @Relationship(type = \u0026#34;friend\u0026#34;) private List\u0026lt;User\u0026gt; friends = new ArrayList\u0026lt;\u0026gt;(); } neo4j教程\nDgraph Dgraph是一个水平可扩展的分布式图形数据库，提供ACID事务，一致的复制和线性化读取。 它是从头开始构建的，用于执行丰富的查询。 作为本机图形数据库，它严格控制数据在磁盘上的排列方式，以优化查询性能和吞吐量，减少磁盘搜索和群集中的网络调用。\nDgraph的目标是提供谷歌生产水平的规模和吞吐量，具有足够低的延迟，可以提供超过数TB的结构化数据的实时用户查询。 Dgraph支持类似GraphQL的查询语法，并通过GRPC和HTTP响应JSON和Protocol Buffers。\n安装 由于Dgraph至少要有一个zero和一个server才能使用，所以我们采用docker-compose.yaml\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 version: \u0026#34;3.2\u0026#34; services: zero: image: dgraph/dgraph:latest volumes: - type: volume source: dgraph target: /dgraph volume: nocopy: true ports: - 5080:5080 - 6080:6080 restart: on-failure command: dgraph zero --my=zero:5080 server: image: dgraph/dgraph:latest volumes: - type: volume source: dgraph target: /dgraph volume: nocopy: true ports: - 8080:8080 - 9080:9080 restart: on-failure command: dgraph alpha --my=server:7080 --lru_mb=2048 --zero=zero:5080 ratel: image: dgraph/dgraph:latest volumes: - type: volume source: dgraph target: /dgraph volume: nocopy: true ports: - 8000:8000 command: dgraph-ratel volumes: dgraph: 实例 依赖 1 compile(\u0026#34;io.dgraph:dgraph4j:1.7.1\u0026#34;) 代码 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 package team.soi.demo; import com.google.gson.Gson; import com.google.gson.annotations.SerializedName; import com.google.protobuf.ByteString; import io.dgraph.DgraphClient; import io.dgraph.DgraphGrpc; import io.dgraph.DgraphProto; import io.dgraph.Transaction; import io.grpc.ManagedChannel; import io.grpc.ManagedChannelBuilder; import lombok.Getter; import lombok.Setter; import java.io.Serializable; import java.time.LocalDateTime; import java.util.*; public class DgraphApp { public static void main(String[] args) { ManagedChannel channel = ManagedChannelBuilder .forAddress(\u0026#34;localhost\u0026#34;, 9080) .usePlaintext() .build(); DgraphGrpc.DgraphStub stub = DgraphGrpc.newStub(channel); DgraphClient dgraph = new DgraphClient(stub); // dgraph.alter(DgraphProto.Operation.newBuilder().setDropAll(true).build()); Gson gson = new Gson(); Transaction tx = dgraph.newTransaction(); try { Tag database = new Tag(); database.setName(\u0026#34;Database\u0026#34;); Tag graph = new Tag(); graph.setName(\u0026#34;Graph\u0026#34;); Category category = new Category(); category.setName(\u0026#34;GraphDB\u0026#34;); Post post = new Post(); post.setTitle(\u0026#34;Hello dgraph\u0026#34;); post.setDigest(\u0026#34;Dgraph demo.\u0026#34;); post.setContent(\u0026#34;Hello dgraph,this is a demo.\u0026#34;); post.setViews(0); post.setCreated(LocalDateTime.now().toString()); post.setTags(Arrays.asList(database, graph)); post.setCategories(Collections.singletonList(category)); String json = gson.toJson(post); DgraphProto.Mutation mu = DgraphProto.Mutation .newBuilder() .setSetJson(ByteString.copyFromUtf8(json)) .build(); tx.mutate(mu); tx.commit(); } finally { tx.discard(); } // query String query = \u0026#34;query posts($offset: int, $count: int){\\n\u0026#34; + \u0026#34;\\t\\tposts(func: has(title), orderdesc: created, orderdesc: views, offset: $offset, first: $count){\\n\u0026#34; + \u0026#34;\\t\\t\\tuid\\n\u0026#34; + \u0026#34;\\t\\t\\ttitle\\n\u0026#34; + \u0026#34;\\t\\t\\tcategories {\\n\u0026#34; + \u0026#34;\\t\\t\\t\\tuid\\n\u0026#34; + \u0026#34;\\t\\t\\t\\tname\\n\u0026#34; + \u0026#34;\\t\\t\\t}\\n\u0026#34; + \u0026#34;\\t\\t\\ttags {\\n\u0026#34; + \u0026#34;\\t\\t\\t\\tuid\\n\u0026#34; + \u0026#34;\\t\\t\\t\\tname\\n\u0026#34; + \u0026#34;\\t\\t\\t}\\n\u0026#34; + \u0026#34;\\t\\t\\tdigest\\n\u0026#34; + \u0026#34;\\t\\t\\tcreated\\n\u0026#34; + \u0026#34;\\t\\t}\\n\u0026#34; + \u0026#34;\\t}\u0026#34;; Map\u0026lt;String, String\u0026gt; vars = new HashMap\u0026lt;\u0026gt;(); vars.put(\u0026#34;$offset\u0026#34;, \u0026#34;0\u0026#34;); vars.put(\u0026#34;$count\u0026#34;, \u0026#34;25\u0026#34;); DgraphProto.Response response = dgraph.newReadOnlyTransaction().queryWithVars(query, vars); Posts posts = gson.fromJson(response.getJson().toStringUtf8(), Posts.class); System.out.printf(\u0026#34;Found hot post: %d\\n\u0026#34;, posts.getPosts().size()); posts.getPosts().forEach(post -\u0026gt; System.out.println(post.getTitle())); } } @Getter @Setter class Posts implements Serializable { List\u0026lt;Post\u0026gt; posts; } @Getter @Setter class Post implements Serializable { @SerializedName(\u0026#34;uid\u0026#34;) private String id; private String title; private String digest; private String content; private String created; private Integer views; private List\u0026lt;Tag\u0026gt; tags = new ArrayList\u0026lt;\u0026gt;(); private List\u0026lt;Category\u0026gt; categories = new ArrayList\u0026lt;\u0026gt;(); } @Getter @Setter class Tag implements Serializable { @SerializedName(\u0026#34;uid\u0026#34;) private String id; private String name; } @Setter @Getter class Category implements Serializable { @SerializedName(\u0026#34;uid\u0026#34;) private String id; private String name; } Dgraph中文文档\n结束语 当然，本次知识粗略介绍了下图数据库，如果有时间，后续我应该会找一个场景来实际应用，先挖个坑，后续努力补上 :smile: 。\n","date":"2019-03-08T11:27:29+08:00","image":"https://ronggle.com/2019/graph-db-getting-and-started/graph_hu3f4d3da678a88c24d39c2337492109c8_2384_120x120_fill_box_smart1_3.png","permalink":"https://ronggle.com/2019/graph-db-getting-and-started/","title":"图形数据库简单介绍"},{"content":"\n如上图，为称之为部署架构。用户使用域名通过LB访问部署在K8S上的应用，LB通过负载均衡反向代理暴露在NodePort上的Traefik服务，Traefik则在K8S内部使用域名发现K8S上通过Ingress暴露出来的应用。\n部署Traefik traefik-rbac.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 --- apiVersion: v1 kind: ConfigMap metadata: name: traefik labels: app: traefik data: traefik.toml: | # traefik.toml logLevel = \u0026#34;INFO\u0026#34; defaultEntryPoints = [\u0026#34;http\u0026#34;, \u0026#34;httpn\u0026#34;] [entryPoints] [entryPoints.http] address = \u0026#34;:80\u0026#34; compress = true [entryPoints.httpn] address = \u0026#34;:8880\u0026#34; compress = true [entryPoints.traefik] address = \u0026#34;:8080\u0026#34; [kubernetes] [traefikLog] format = \u0026#34;json\u0026#34; [api] entryPoint = \u0026#34;traefik\u0026#34; dashboard = true --- apiVersion: apps/v1 kind: Deployment metadata: name: traefik labels: app: traefik spec: replicas: 1 selector: matchLabels: app: traefik release: traefik template: metadata: annotations: checksum/config: 55a29204986001f01835269242a08a68d59bc276658728cc28c5543e34f26d0b labels: app: traefik spec: serviceAccountName: traefik terminationGracePeriodSeconds: 60 containers: - image: traefik:1.7.4 name: traefik resources: requests: cpu: \u0026#34;100m\u0026#34; memory: \u0026#34;20Mi\u0026#34; limits: cpu: \u0026#34;100m\u0026#34; memory: \u0026#34;30Mi\u0026#34; readinessProbe: tcpSocket: port: 80 failureThreshold: 1 initialDelaySeconds: 10 periodSeconds: 10 successThreshold: 1 timeoutSeconds: 2 livenessProbe: tcpSocket: port: 80 failureThreshold: 3 initialDelaySeconds: 10 periodSeconds: 10 successThreshold: 1 timeoutSeconds: 2 volumeMounts: - mountPath: /config name: config ports: - name: http containerPort: 80 protocol: TCP - name: httpn containerPort: 8880 protocol: TCP - name: https containerPort: 443 protocol: TCP - name: dash containerPort: 8080 protocol: TCP args: - --configfile=/config/traefik.toml volumes: - name: config configMap: name: traefik --- apiVersion: v1 kind: Service metadata: name: traefik labels: app: traefik annotations: spec: type: NodePort selector: app: traefik release: traefik ports: - port: 80 name: http targetPort: http - port: 443 name: https targetPort: httpn --- kind: ServiceAccount apiVersion: v1 metadata: name: traefik --- kind: ClusterRole apiVersion: rbac.authorization.k8s.io/v1 metadata: name: traefik rules: - apiGroups: - \u0026#34;\u0026#34; resources: - pods - services - endpoints - secrets verbs: - get - list - watch - apiGroups: - extensions resources: - ingresses verbs: - get - list - watch - apiGroups: - extensions resources: - ingresses/status verbs: - update --- kind: ClusterRoleBinding apiVersion: rbac.authorization.k8s.io/v1 metadata: name: traefik roleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: traefik subjects: - kind: ServiceAccount name: traefik namespace: kube-system 1 $ kubectl apply -f traefik-rbac.yaml -n traefik traefik-ui.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 --- apiVersion: extensions/v1beta1 kind: Ingress metadata: name: traefik-dashboard labels: app: traefik annotations: spec: rules: - host: traefik.cloud-labs.io http: paths: - backend: serviceName: traefik-dashboard servicePort: 80 --- apiVersion: v1 kind: Service metadata: name: traefik-dashboard labels: app: traefik annotations: spec: selector: app: traefik release: traefik ports: - port: 80 targetPort: 8080 1 $ kubectl apply -f traefik-ui.yaml -n traefik LB 这边将使用nginx作为LB，使用其的反向代理，将请求反向代理到traefik所在的NodePort上，nginx配置default.conf如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 upstream k8s { server 192.168.100.100:30080; server 192.168.100.101:30080; server 192.168.100.102:30080; } server { listen 80; server_name cloud-labs.io; #access_log logs/quancha.access.log main; #error_log logs/quancha.error.log; root html; index index.html index.htm index.php; ## send request back to apache ## location / { proxy_pass http://k8s; #Proxy Settings proxy_redirect off; proxy_set_header Host $host; proxy_set_header X-Real-IP $remote_addr; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; proxy_next_upstream error timeout invalid_header http_500 http_502 http_503 http_504; proxy_max_temp_file_size 0; proxy_connect_timeout 90; proxy_send_timeout 90; proxy_read_timeout 90; proxy_buffer_size 4k; proxy_buffers 4 32k; proxy_busy_buffers_size 64k; proxy_temp_file_write_size 64k; } } 配置hosts 1 2 3 $ cat /etc/hosts 192.168.100.100 traefik.cloud-labs.io 验证 打开浏览器，输入traefik.cloud-labs.io可以看到如下： 部署一个应用 为了简单方便起见，我在这里将部署一个nginx，并使用域名nginx.cloud-labs.io来访问它。配置文件nginx-apps.yaml如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 --- apiVersion: apps/v1 kind: Deployment metadata: name: nginx labels: app: nginx role: web spec: replicas: 1 selector: matchLabels: app: nginx template: metadata: labels: app: nginx spec: containers: - name: nginx image: nginx:latest imagePullPolicy: IfNotPresent ports: - name: http containerPort: 80 protocol: TCP - name: https containerPort: 443 protocol: TCP --- apiVersion: v1 kind: Service metadata: name: nginx labels: app: nginx annotations: spec: type: ClusterIP selector: app: nginx ports: - port: 80 name: http targetPort: http --- apiVersion: extensions/v1beta1 kind: Ingress metadata: name: nginx labels: app: nginx annotations: spec: rules: - host: nginx.cloud-labs.io http: paths: - backend: serviceName: nginx servicePort: 80 1 2 3 4 5 $ kubectl apply -f nginx-apps.yaml $ cat /etc/hosts 192.168.100.100 traefik.cloud-labs.io 192.168.100.100 nginx.cloud-labs.io 打开浏览器http://nginx.cloud-labs.io/： 结束语 一篇水文，到此结束，本文重点记录了在kubernetes上使用traefik作为ingress使用，不为别的，就是为了记录下。\n","date":"2018-11-10T21:34:29+08:00","image":"https://ronggle.com/2018/traefik-on-kubernetes/traefik.svg","permalink":"https://ronggle.com/2018/traefik-on-kubernetes/","title":"Traefik on Kubernetes"},{"content":"前提 Consul Docker Golang 服务注册 首先，我们来定义服务注册的接口：\n1 2 3 4 5 // Registry interface for extend type Registry interface { Register(id string, name string, port int, tags ...string) error DeRegister(string) error } 服务注册主要有两个接口，一个用于注册服务的Register，一个用于取消注册的DeRegister,当我们注册服务的时候，我们需要提供当前服务的ID，名称，端口等信息。\n注册中心Consul 为了方便我们的服务注册，我则使用Consul作为注册中心，如果你还不知道什么是Consul，你可以看我之前的博客consul学习一-初见以及Consul的官网进行了解。\n实现Registry：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 type consul struct { client *api.Client addr string } // NewConsulRegistry returns a registryClient interface for given consul address func NewConsulRegistry(c Config) (Registry, error) { addr := fmt.Sprintf(\u0026#34;%s:%s\u0026#34;, c.Host, c.Port) if addr == \u0026#34;\u0026#34; { addr = \u0026#34;consul:8500\u0026#34; } cfg := api.DefaultConfig() cfg.Address = addr cl, err := api.NewClient(cfg) if err != nil { logrus.Errorf(\u0026#34;Can\u0026#39;t connect to consul server at %s\u0026#34;, addr) return nil, err } return consul{client: cl, addr: addr}, nil } func (r consul) Register(id string, name string, port int, tags ...string) error { conn, err := net.Dial(\u0026#34;udp\u0026#34;, \u0026#34;8.8.8.8:80\u0026#34;) if err != nil { return fmt.Errorf(\u0026#34;unable to determine local addr: %v\u0026#34;, err) } defer conn.Close() localAddr := conn.LocalAddr().(*net.UDPAddr) asr := \u0026amp;api.AgentServiceRegistration{ ID: name, Name: name, Port: port, EnableTagOverride: false, Tags: tags, Address: localAddr.IP.String(), } err = r.client.Agent().ServiceRegister(asr) if err != nil { logrus.Errorf(\u0026#34;Failed to register service at \u0026#39;%s\u0026#39;. error: %v\u0026#34;, r.addr, err) } else { logrus.Infof(\u0026#34;Regsitered service \u0026#39;%s\u0026#39; at consul.\u0026#34;, id) } return err } func (r consul) DeRegister(name string) error { err := r.client.Agent().ServiceDeregister(name) if err != nil { logrus.Errorf(\u0026#34;Failed to deregister service by id: \u0026#39;%s\u0026#39;. Error: %v\u0026#34;, name, err) } else { logrus.Infof(\u0026#34;Deregistered service \u0026#39;%s\u0026#39; at consul.\u0026#34;, name) } return err } 完整代码可以查看gf\n服务发现 同样，我们定义一个接口：\n1 2 3 4 // Discovery service type Discovery interface { Dial(name string, opts ...grpc.DialOption) (*grpc.ClientConn, error) } 基于Consul发现服务 基于Consul发现服务其实简单，就是使用Consul提供的API，我们去注册中心读取服务的信息（IP，port等）。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 // discovery provider type discovery struct { *api.Client dialopts []grpc.DialOption } // NewConsulDiscovery returns discovery func NewConsulDiscovery(cfg Config) (Discovery, error) { config := api.DefaultConfig() config.Address = fmt.Sprintf(\u0026#34;%s:%s\u0026#34;, cfg.Host, cfg.Port) c, err := api.NewClient(config) if err != nil { return nil, err } opts := []grpc.DialOption{ grpc.WithInsecure(), grpc.WithBlock(), } if cfg.Tracer != nil { opts = append(opts, grpc.WithUnaryInterceptor(otgrpc.OpenTracingClientInterceptor(cfg.Tracer))) opts = append(opts, grpc.WithStreamInterceptor(otgrpc.OpenTracingStreamClientInterceptor(cfg.Tracer))) } return discovery{c, opts}, nil } // Dial grpc server func (c discovery) Dial(name string, opts ...grpc.DialOption) (*grpc.ClientConn, error) { r, err := lb.NewResolver(c.Client, name, \u0026#34;\u0026#34;) if err != nil { return nil, fmt.Errorf(\u0026#34;Create balancer resolver for service %s failed. Error: %v\u0026#34;, name, err) } c.dialopts = append(c.dialopts, grpc.WithBalancer(grpc.RoundRobin(r))) c.dialopts = append(c.dialopts, opts...) conn, err := grpc.Dial(\u0026#34;\u0026#34;, c.dialopts...) if err != nil { return nil, fmt.Errorf(\u0026#34;Failed to dial %s: %v\u0026#34;, name, err) } return conn, nil } 完整代码可以查看gf\n使用 注册与取消注册 根据上篇中，我们只需要在启动gRPC server的时候，把我们的服务注册到Consul即可：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 package cmd import ( \u0026#34;github.com/ycrxun/add\u0026#34; \u0026#34;github.com/ycrxun/add/server\u0026#34; \u0026#34;github.com/ycrxun/gf/registry\u0026#34; \u0026#34;github.com/sirupsen/logrus\u0026#34; \u0026#34;github.com/spf13/cobra\u0026#34; \u0026#34;google.golang.org/grpc\u0026#34; \u0026#34;github.com/ycrxun/gf\u0026#34; ) var serveCmd = \u0026amp;cobra.Command{ Use: \u0026#34;serve\u0026#34;, Short: \u0026#34;Run the RPC server\u0026#34;, Run: func(cmd *cobra.Command, args []string) { logrus.Fatal(runServe()) }, } func runServe() error { r, err := registry.NewRegistry(registry.Config{ Provider: registry.Consul, Host: \u0026#34;192.168.31.70\u0026#34;, Port: \u0026#34;8500\u0026#34;, }) if err != nil { return err } s := gf.NewService(\u0026#34;add\u0026#34;) s.GRPCImplementation(func(g *grpc.Server) { add.RegisterAddServiceServer(g, server.AddServer{}) }) s.UseRegistry(r) s.Run() return nil } func init() { rootCmd.AddCommand(serveCmd) } 发现服务 通过上面的代码改造，我们会在服务端启动时把服务注册到Consul，那么在客户端使用时，我们只需要去Consul发现服务即可。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 package cmd import ( \u0026#34;context\u0026#34; \u0026#34;github.com/kataras/iris\u0026#34; \u0026#34;github.com/sirupsen/logrus\u0026#34; \u0026#34;github.com/spf13/cobra\u0026#34; \u0026#34;github.com/ycrxun/add\u0026#34; \u0026#34;github.com/ycrxun/gf/discovery\u0026#34; ) var clientCmd = \u0026amp;cobra.Command{ Use: \u0026#34;client\u0026#34;, Short: \u0026#34;Run the RPC client\u0026#34;, Run: func(cmd *cobra.Command, args []string) { logrus.Fatal(runClient()) }, } func runClient() error { // Define discovery d, err := discovery.NewDiscovery(discovery.Config{ Provider: discovery.Consul, Host: \u0026#34;192.168.31.70\u0026#34;, Port: \u0026#34;8500\u0026#34;, }) if err != nil { return err } // discovery service via name. cc, err := d.Dial(\u0026#34;add\u0026#34;) if err != nil { return err } cl := add.NewAddServiceClient(cc) app := iris.New() app.Get(\u0026#34;/:a/:b\u0026#34;, func(ctx iris.Context) { a, _ := ctx.Params().GetInt64(\u0026#34;a\u0026#34;) b, _ := ctx.Params().GetInt64(\u0026#34;b\u0026#34;) c := context.Background() rs, err := cl.Add(c, \u0026amp;add.AddRequest{A: uint64(a), B: uint64(b)}) if err != nil { ctx.Text(err.Error()) return } ctx.JSON(rs) }) return app.Run(iris.Addr(\u0026#34;:8100\u0026#34;)) } func init() { rootCmd.AddCommand(clientCmd) } 启动 编译并依次启动\n1 2 3 4 5 6 7 8 $ pwd /home/soi/golang/src/github.com/ycrxun/add/add $ go build $ ./add serve INFO[0000] add serve at 0.0.0.0:8000 INFO[0000] Regsitered service \u0026#39;add\u0026#39; at consul. 此时，我们打开consul的控制面板，可以看到服务已经注册到上面了 在另一个终端启动client\n1 2 3 $ ./add client Now listening on: http://localhost:8100 Application started. Press CTRL+C to shut down. 打开浏览器，输入http://localhost:8100/1/1\n1 {\u0026#34;result\u0026#34;:2} ","date":"2018-08-12T20:04:08+08:00","image":"https://ronggle.com/2018/registry-microservies-via-grpc/consul_hu3856c57a2da839c1c05d9a86cb7503bb_22688_120x120_fill_box_smart1_3.png","permalink":"https://ronggle.com/2018/registry-microservies-via-grpc/","title":"使用gRPC开发微服务-服务注册与发现"},{"content":"什么是gRPC gRPC是一个现代的开源高性能RPC框架，可以在任何环境中运行。它可以有效地连接数据中心内和跨数据中心的服务，并提供可插拔的支持，以实现负载平衡，跟踪，健康检查和身份验证。它支持多种语言。\ngRPC使用Protocol Buffers作为IDL定义服务，Protocol Buffers是一个功能强大的二进制序列化工具集和语言。\n主要使用场景 在微服务式架构中有效地连接多语言服务 将移动设备，浏览器客户端连接到后端服务 生成高效的客户端库 核心功能 10种语言的客户端库 高效和简单的服务定义框架 基于http/2的传输的双向流 可插拔身份验证，跟踪，负载平衡和健康检查 安装和前期准备请参阅官方文档。\n前置条件 Protocol Buffers Golang 一个简单的开始 使用Protocol Buffers定义服务 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 syntax = \u0026#34;proto3\u0026#34;; package add; message AddRequest { uint64 a = 1; uint64 b = 2; } message AddResponse { uint64 result = 1; } service AddService { rpc Add (AddRequest) returns (AddResponse) {} } 项目结构 1 2 3 4 5 6 7 8 9 10 11 . ├── add │ ├── cmd │ │ ├── client.go │ │ ├── root.go │ │ └── server.go │ └── main.go ├── add.pb.go ├── add.proto └── server └── server.go 生成代码 1 2 3 4 $ pwd /home/soi/golang/src/github.com/ycrxun/add $ protoc -I . add.proto --go_out=plugins=grpc:. 当我们执行完命令后，会在当前文件夹上生成一个add.pb.go文件，这个文件我们不需要去动，但是我们生成的这个文件中有我们定义的AddServiceServer的接口，需要我们去实现。\n1 2 3 4 // AddServiceServer is the server API for AddService service. type AddServiceServer interface { Add(context.Context, *AddRequest) (*AddResponse, error) } 实现AddServiceServer 1 2 3 4 5 6 $ pwd /home/soi/golang/src/github.com/ycrxun/add $ mkdir server $ touch server/server.go 然后在server.go中定义一个structAddServer,并且实现AddServiceServer的Add方法。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 package server import ( \u0026#34;context\u0026#34; \u0026#34;github.com/ycrxun/add\u0026#34; ) // AddServer struct for AddService type AddServer struct { } // Add method func (a AddServer) Add(ctx context.Context, r *add.AddRequest) (*add.AddResponse, error) { rs := r.A + r.B s := add.AddResponse{ Result: uint64(rs), } return \u0026amp;s, nil } 到此，我们已经实现了我们的业务逻辑部分，但是并不能对外提供服务，所以我们要完成我们的gRPC的server。\n实现Server端 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 $ pwd /home/soi/golang/src/github.com/ycrxun/add/add $ ls cmd main.go $ cat main.go package main import \u0026#34;github.com/ycrxun/add/add/cmd\u0026#34; func main() { cmd.Execute() } $ cat cmd/root.go package cmd import ( \u0026#34;fmt\u0026#34; \u0026#34;os\u0026#34; \u0026#34;github.com/spf13/cobra\u0026#34; ) var rootCmd = \u0026amp;cobra.Command{ Use: \u0026#34;add\u0026#34;, Short: \u0026#34;grpc add service simple.\u0026#34;, } // Execute cmd func Execute() { if err := rootCmd.Execute(); err != nil { fmt.Println(err) os.Exit(-1) } } 为了后续的操作，我们最好按照这种结构去写，当然，你也可以不这么写。\n接下来，我们去实现我们的gRPC Server\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 package cmd import ( \u0026#34;net\u0026#34; \u0026#34;github.com/ycrxun/add\u0026#34; \u0026#34;github.com/ycrxun/add/server\u0026#34; \u0026#34;github.com/sirupsen/logrus\u0026#34; \u0026#34;github.com/spf13/cobra\u0026#34; \u0026#34;google.golang.org/grpc\u0026#34; ) var serveCmd = \u0026amp;cobra.Command{ Use: \u0026#34;serve\u0026#34;, Short: \u0026#34;Run the RPC server\u0026#34;, Run: func(cmd *cobra.Command, args []string) { logrus.Fatal(runServe()) }, } func runServe() error { lis, err := net.Listen(\u0026#34;tcp\u0026#34;, \u0026#34;:8000\u0026#34;) logrus.Infof(\u0026#34;add serve at %s\u0026#34;, \u0026#34;0.0.0.0:8000\u0026#34;) if err != nil { return err } s := grpc.NewServer() add.RegisterAddServiceServer(s, \u0026amp;server.AddServer{}) return s.Serve(lis) } func init() { rootCmd.AddCommand(serveCmd) } 至此，我们的gRPC server以及我们的业务逻辑就完成了。\n是不是很鸡冻呢？\n接下来我们来编译并运行起来。\n1 2 3 4 5 6 7 8 9 10 $ pwd /home/soi/golang/src/github.com/ycrxun/add/add $ go build $ ls add cmd main.go $ ./add serve INFO[0000] add serve at 0.0.0.0:8000 编写客户端 上面我们已经编写并完成了gRPC server及其业务逻辑，并可以对外提供服务了，这时候我们需要使用一个客户端去调用我们的服务，为了方便测试，我们选择使用iris来构建一个HTTP的API对外提供服务。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 package cmd import ( \u0026#34;context\u0026#34; \u0026#34;github.com/kataras/iris\u0026#34; \u0026#34;github.com/sirupsen/logrus\u0026#34; \u0026#34;github.com/spf13/cobra\u0026#34; \u0026#34;github.com/ycrxun/add\u0026#34; \u0026#34;google.golang.org/grpc\u0026#34; ) var clientCmd = \u0026amp;cobra.Command{ Use: \u0026#34;client\u0026#34;, Short: \u0026#34;Run the RPC client\u0026#34;, Run: func(cmd *cobra.Command, args []string) { logrus.Fatal(runClient()) }, } func runClient() error { // 建立一个grpc连接 cc, err := grpc.Dial(\u0026#34;0.0.0.0:8000\u0026#34;, grpc.WithInsecure()) if err != nil { return err } // 创建AddService的客户端 cl := add.NewAddServiceClient(cc) app := iris.New() app.Get(\u0026#34;/:a/:b\u0026#34;, func(ctx iris.Context) { a, _ := ctx.Params().GetInt64(\u0026#34;a\u0026#34;) b, _ := ctx.Params().GetInt64(\u0026#34;b\u0026#34;) c := context.Background() rs, err := cl.Add(c, \u0026amp;add.AddRequest{A: uint64(a), B: uint64(b)}) if err != nil { ctx.Text(err.Error()) return } ctx.JSON(rs) }) return app.Run(iris.Addr(\u0026#34;:8100\u0026#34;)) } func init() { rootCmd.AddCommand(clientCmd) } 到此，我们的客户端也完成了，接下来为们重新编译下，并依次将服务端，客户端启动起来。\n1 2 3 4 5 6 7 $ pwd /home/soi/golang/src/github.com/ycrxun/add/add $ go build $ ./add serve INFO[0000] add serve at 0.0.0.0:8000 重新开一个终端\n1 2 3 $ ./add client Now listening on: http://localhost:8100 Application started. Press CTRL+C to shut down. 打开浏览器，输入http://localhost:8100/1/1\n1 {\u0026#34;result\u0026#34;:2} ","date":"2018-08-12T17:44:33+08:00","image":"https://ronggle.com/2018/setup-microservies-via-grpc/grpc.svg","permalink":"https://ronggle.com/2018/setup-microservies-via-grpc/","title":"使用gRPC开发微服务-开始"},{"content":"目标 开启Traefik的https服务。 前期准备 Docker Traefik Portainer Docker Compose Docker Swarm Traefik Traefik云原生边缘路由器,反向代理/负载均衡器，简单，动态，自动，快速，功能齐全，开源，经过生产验证，提供指标，并与各种主要集群技术集成\u0026hellip;\u0026hellip;难怪它如此受欢迎！\n新建stack编排文件 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 version: \u0026#34;3.4\u0026#34; services: server: image: traefik:latest command: - \u0026#34;--api\u0026#34; - \u0026#34;--api.dashboard\u0026#34; - \u0026#34;--api.statistics\u0026#34; - \u0026#34;--entrypoints=Name:http Address::80 Redirect.EntryPoint:https\u0026#34; - \u0026#34;--entrypoints=Name:https Address::443 TLS\u0026#34; - \u0026#34;--defaultentrypoints=http,https\u0026#34; - \u0026#34;--acme\u0026#34; - \u0026#34;--acme.storage=/acme.json\u0026#34; - \u0026#34;--acme.entryPoint=https\u0026#34; - \u0026#34;--acme.httpChallenge.entryPoint=http\u0026#34; - \u0026#34;--acme.onHostRule=true\u0026#34; - \u0026#34;--acme.onDemand=false\u0026#34; - \u0026#34;--acme.email=example@mail.com\u0026#34; - \u0026#34;--docker\u0026#34; - \u0026#34;--docker.swarmMode\u0026#34; - \u0026#34;--docker.domain=example.com\u0026#34; - \u0026#34;--docker.watch\u0026#34; ports: - \u0026#34;80:80\u0026#34; - \u0026#34;443:443\u0026#34; networks: - proxy volumes: - /var/run/docker.sock:/var/run/docker.sock deploy: placement: constraints: - node.role == manager update_config: parallelism: 1 delay: 10s restart_policy: condition: on-failure labels: - traefik.backend=traefik - traefik.frontend.rule=Host:monitor.example.com - traefik.docker.network=traefik_proxy - traefik.port=8080 networks: proxy: 如上，我们开启了traefik的api、docker、docker swarm、https、自动向Let's Encrypt申请证书功能，并且把它的web功能暴露到monitor.example.com，当我们部署完成后，就可以使用https://monitor.example.com进行访问。\n部署traefik 部署其实很简单，为采用的stack模式部署的traefik，命令如下：\n1 2 3 4 5 6 7 $ ls traefik.yml $ sudo docker stack deploy -c traefik.yml traefik 稍等一会儿就可以查看到我们已经完成了traefik的部署，并且使用traefik自动发现Docker上运行的应用时，以及是https了，查看证书详情，可以看到我们的证书使用的Let\u0026#39;s Encrypt申请。 ![证书详情](lets-encrypt.png) ","date":"2018-07-26T22:42:22+08:00","image":"https://ronggle.com/2018/traefik-https/traefik.svg","permalink":"https://ronggle.com/2018/traefik-https/","title":"开启Traefik的https"},{"content":" ","date":"2018-05-17T21:08:52+08:00","image":"https://ronggle.com/2018/2b%E5%B0%8F%E5%A7%90%E5%A7%90/2_hu4ff3dc0d17a748918c227dbf7b7da5e2_8189502_120x120_fill_q75_box_smart1.jpg","permalink":"https://ronggle.com/2018/2b%E5%B0%8F%E5%A7%90%E5%A7%90/","title":"2B小姐姐"},{"content":"那么，既然回归，当然是要拿出干货咯。\n今天说点什么呢？沉寂了这么久，也一直在做与Docker相关的工作与学习，就记录下最近的一些内容，主要是为了防止自己又忘了，每次都要去找各种文档，没法快速复制出一份自己玩过的东西。\n目标 使用Docker搭建一个自己的”云平台“，大概就是，可以在上面部署应用，并且自动暴露出来。\n前期准备 Docker Traefik Portainer Docker Compose Docker Swarm 以上是我们需要前期准备好的，包括它们分别是什么、怎么安装、怎么使用？\n因为篇幅问题，在此不一一详细节介绍，如果你感兴趣，希望你能够自己通过搜索引擎自行了解，如果有什么问题，你可以联系我。\nTraefik Træfɪk 是一个为了让部署微服务更加便捷而诞生的现代HTTP反向代理、负载均衡工具。 它支持多种后台 (Docker, Swarm, Kubernetes, Marathon, Mesos, Consul, Etcd, Zookeeper, BoltDB, Rest API, file…) 来自动化、动态的应用它的配置文件设置。\n我们用它作什么呢？没错，反向代理和负载均衡，同时因为其提供了好用的动态功能，使得我们可以用它配置域名动态的功能。\n怎么理解呢？假设我现在有了一个域名cloud-labs.io，然后为使用这个域名配置了一个“云平台”，然后为希望我在这个平台部署一个应用nina,部署完成后，为就可以使用nina.cloud-labs.io进行访问，并不在乎我的运用被具体部署到哪里。\n那么，要怎么做呢？\n如果使用了traefik，其实很简单，我们只需要将应用作为一个backend暴露给traefik即可。如果结合Docker使用，为们只需要在Label上加上对应的配置即可。\n下面我们一起来部署traefik吧！\n前提条件，你已经安装号Docker以及初始化完成Docker Swarm，当然你也可以只使用Docker模式，你可以参考traefik的官方文档。\n新建stack编排文件 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 version: \u0026#39;3.3\u0026#39; services: proxy: image: traefik command: --web --docker --docker.domain=cloud-labs.io --docker.watch --docker.swarmmode=true --logLevel=DEBUG deploy: labels: - traefik.backend=traefik - traefik.frontend.rule=Host:monitor.cloud-labs.io - traefik.port=8080 - traefik.docker.network=traefik_proxy networks: - proxy ports: - \u0026#34;80:80\u0026#34; - \u0026#34;443:443\u0026#34; volumes: - /var/run/docker.sock:/var/run/docker.sock - /dev/null:/traefik.toml networks: proxy 如上，我们开启了traefik的web、docker、docker swarm功能，并且把它的web功能暴露到monitor.cloud-labs.io，当我们部署完成后，就可以使用monitor.cloud-labs.io进行访问。\n部署traefik 部署其实很简单，为采用的stack模式部署的traefik，命令如下：\n1 2 3 4 $ ls traefik.yml $ docker stack deploy -c traefik.yml traefik 稍等一会儿，就可以在使用monitor.cloud-labs.io访问了。\n当然，monitor.cloud-labs.io是一个假定的域名，需要在你的host文件中进行配置。 $ cat /etc/hosts 127.0.0.1 monitor.cloud-labs.io\nPortainer Portainer是一个轻量级管理用户界面，可让你轻松管理不同的Docker环境（Docker主机或Swarm集群）。其使用与部署一样简单，它包含一个可以在任何Docker引擎上运行的容器（可以作为Linux容器或Windows本机容器部署）。并允许你管理Docker容器，图像，卷，网络等等！它与独立的Docker引擎和Docker Swarm模式兼容。\n正如它的介绍那般，为们可以使用它作为我们的“云平台”的控制台，来作为我们平时的应用部署、管理、配置的WebUI，让我们的操作更加人性化。\n新建stack编排文件 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 version: \u0026#39;3.3\u0026#39; services: server: image: portainer/portainer command: -H unix:///var/run/docker.sock networks: - traefik_proxy deploy: labels: - traefik.backend=portainer - traefik.frontend.rule=Host:console.cloud-labs.io - traefik.docker.network=traefik_proxy - traefik.port=9000 volumes: - /var/run/docker.sock:/var/run/docker.sock - portainer:/data networks: traefik_proxy: external: true volumes: portainer 如上，为们将会把Portainer部署到Docker环境中，同时将其暴露到了traefik中，使用console.cloud-labs.io进行访问。在这里，你可能会发现，为们并没有为portainer配置暴露的端口，这也是因为traefik给我们带来的一个好处，便是为们不需要在我们的物理机上开启那么多的端口映射，为们知道，如果端口映射过多时，我们会在难以管理、感知，往往会出现端口已经使用的情况下导致部署失败。\n值得注意的是，我们的Portainer的网络设置：\n1 2 3 networks: traefik_proxy: external: true 是的，我们使用的是traefik的网络，因为我们希望它能够被traefik发现，并作反向代理和负载均衡。\n部署Portainer 部署Portainer也很简单，为们使用stack进行部署：\n1 2 3 4 $ ls portainer.yml $ docker stack deploy -c portainer.yml portainer 稍等一会儿，就可以在使用monitor.cloud-labs.io中发现我们的应用以及部署好了，并且被暴露在console.cloud-labs.io。\n同样的，console.cloud-labs.io是一个假定的域名，需要在你的host文件中进行配置。 $ cat /etc/hosts 127.0.0.1 console.cloud-labs.io\n部署运用 上面，我们已经把我们“云平台”的反向代理、负载均衡组件traefik，以及控制台portainer部署完毕，那么我们接下来就使用我们的“云平台”来部署一个应用吧！\n接下来，我们来部署一个比较简单的应用。\n登入控制台 登陆我们“云平台”的控制台后，选择stack，为们来编排一下我们的应用。\n进入Traefik 当我们在控制台部署完成我们的gitea后，我们可以在Traefik的WebUI中看到，我们的gitea已经部署完成，并暴露在git.cloud-labs.io，这时我们可以访问git.cloud-labs.io来进行gitea的配置。\n应用预览 配置完成gitea后我们可以看到如下：\n总结 通过这次实战，你是否有收获呢？如果有，请为自己鼓掌，如果没有，那你也别责怪我，因为我其实真的是为了让自己记住。\n好吧，我会把最后的文件整理下，放到我的Github上，仓库为cloud-labs。\n","date":"2018-05-13T18:00:59+08:00","image":"https://ronggle.com/2018/cloud-labs/monitor_hub4210f43b5c4a58a23469c3591ba2df9_54092_120x120_fill_box_smart1_3.png","permalink":"https://ronggle.com/2018/cloud-labs/","title":"使用Docker打造自己的云平台"},{"content":"环境准备 节点名称 ip 角色 ams1 192.168.42.33 leader ams2 192.168.42.198 server ams3 192.168.42.85 server ams4 192.168.42.252 client 安装并配置 安装 分别在4个节点安装\n1 2 3 4 5 $ wget https://releases.hashicorp.com/consul/1.0.2/consul_1.0.2_linux_amd64.zip $ unzip onsul_1.0.2_linux_amd64.zip -d /usr/bin $ consul version Consul v1.0.2 Protocol 2 spoken by default, understands 2 to 3 (agent will automatically use protocol \u0026gt;2 when speaking to compatible agents) Consul安装之后，运行Agent。 Agent可以在server或client模式下运行。每个数据中心都必须至少有一个运行于serve，但推荐使用3台或5台服务器。 启动leader 创建配置文件 1 2 3 4 5 6 7 8 9 10 11 # cat /etc/consul.d/server.json { \u0026#34;datacenter\u0026#34;: \u0026#34;ams\u0026#34;, \u0026#34;data_dir\u0026#34;: \u0026#34;/data/consul\u0026#34;, \u0026#34;node_name\u0026#34;: \u0026#34;consul-leader\u0026#34;, \u0026#34;server\u0026#34;: true, \u0026#34;ui\u0026#34;: true, \u0026#34;bootstrap_expect\u0026#34;: 1, \u0026#34;client_addr\u0026#34;: \u0026#34;0.0.0.0\u0026#34;, \u0026#34;advertise_addr\u0026#34;: \u0026#34;192.168.42.33\u0026#34; } 数据目录 1 2 # mkdir -p /data/consul # mkdir -p /data/consul/log 启动 1 nohup consul agent -config-dir /etc/consul.d \u0026gt; /data/consul/log/server.log 2\u0026gt;\u0026amp;1 \u0026amp; 启动server1 创建配置文件 1 2 3 4 5 6 7 8 9 10 11 # cat /etc/consul.d/server.json { \u0026#34;datacenter\u0026#34;: \u0026#34;ams\u0026#34;, \u0026#34;data_dir\u0026#34;: \u0026#34;/data/consul\u0026#34;, \u0026#34;node_name\u0026#34;: \u0026#34;consul-server1\u0026#34;, \u0026#34;server\u0026#34;: true, \u0026#34;ui\u0026#34;: true, \u0026#34;client_addr\u0026#34;: \u0026#34;0.0.0.0\u0026#34;, \u0026#34;advertise_addr\u0026#34;: \u0026#34;192.168.42.198\u0026#34;, \u0026#34;retry_join\u0026#34;: [\u0026#34;192.168.42.33\u0026#34;] } 数据目录 1 2 # mkdir -p /data/consul # mkdir -p /data/consul/log 启动 1 nohup consul agent -config-dir /etc/consul.d \u0026gt; /data/consul/log/server.log 2\u0026gt;\u0026amp;1 \u0026amp; 启动server2 创建配置文件 1 2 3 4 5 6 7 8 9 10 11 # cat /etc/consul.d/server.json { \u0026#34;datacenter\u0026#34;: \u0026#34;ams\u0026#34;, \u0026#34;data_dir\u0026#34;: \u0026#34;/data/consul\u0026#34;, \u0026#34;node_name\u0026#34;: \u0026#34;consul-server2\u0026#34;, \u0026#34;server\u0026#34;: true, \u0026#34;ui\u0026#34;: true, \u0026#34;client_addr\u0026#34;: \u0026#34;0.0.0.0\u0026#34;, \u0026#34;advertise_addr\u0026#34;: \u0026#34;192.168.42.85\u0026#34;, \u0026#34;retry_join\u0026#34;: [\u0026#34;192.168.42.33\u0026#34;] } 数据目录 1 2 # mkdir -p /data/consul # mkdir -p /data/consul/log 启动 1 nohup consul agent -config-dir /etc/consul.d \u0026gt; /data/consul/log/server.log 2\u0026gt;\u0026amp;1 \u0026amp; 启动client 创建配置文件 1 2 3 4 5 6 7 8 9 10 # cat /etc/consul.d/server.json { \u0026#34;datacenter\u0026#34;: \u0026#34;ams\u0026#34;, \u0026#34;data_dir\u0026#34;: \u0026#34;/data/consul\u0026#34;, \u0026#34;node_name\u0026#34;: \u0026#34;consul-client\u0026#34;, \u0026#34;server\u0026#34;: true, \u0026#34;client_addr\u0026#34;: \u0026#34;0.0.0.0\u0026#34;, \u0026#34;advertise_addr\u0026#34;: \u0026#34;192.168.42.252\u0026#34;, \u0026#34;retry_join\u0026#34;: [\u0026#34;192.168.42.33\u0026#34;] } 数据目录 1 2 # mkdir -p /data/consul # mkdir -p /data/consul/log 启动 1 nohup consul agent -config-dir /etc/consul.d \u0026gt; /data/consul/log/server.log 2\u0026gt;\u0026amp;1 \u0026amp; 验证安装 在浏览器中打开http://192.168.42.33:8500\n","date":"2018-01-15T21:42:19+08:00","image":"https://ronggle.com/2018/consul-cluster/consul-cluster-ui_hu839dc4e3ff387a820fa0d8e23d0db694_38705_120x120_fill_box_smart1_3.png","permalink":"https://ronggle.com/2018/consul-cluster/","title":"consul学习二-集群"},{"content":"什么是Consul Consul 是HashiCorp公司推出的一款开源工具，用于实现分布式系统的服务发现与配置。它提供了几个关键功能：\n服务发现：Consul的客户端可以为一些客户端发现给定服务的提供者，诸如api或mysql之类的服务，通过使用DNS或HTTP，应用程序可以很容易地找到它们所依赖的服务。 健康检查：Consul的客户端可以提供任何数量的健康检查，或者与给定的服务（“是Web服务器返回200 OK”），或与本地节点（“内存利用率低于90％”）相关联。操作员可以使用此信息来监视群集运行状况，服务发现组件使用此信息将流量从不健康的主机中引导出去。 KV存储：应用程序可以使用Consul的分层键/值存储，用于任何目的，包括动态配置，功能标记，协调，领导选举等等。简单的HTTP API使其易于使用。 多数据中心：Consul支持多个数据中心。这意味着Consul的用户不必担心构建额外的抽象层以扩展到多个区域。 基本架构 Consul是一个分布式，高度可用的系统。 向Consul提供服务的每个节点都运行一个Consul代理。 发现其他服务或获取/设置键/值数据不需要运行代理。 代理负责健康检查节点上的服务以及节点本身。\n代理与一个或多个Consul服务器通讯。 Consul服务器是数据存储和复制的地方。 服务器自己选出一位领导。 虽然Consul可以在一台服务器上运行，但推荐使用3到5来避免导致数据丢失的故障情况。 每个数据中心都建议使用一组Consul服务器。\n需要发现其他服务或节点的基础架构组件可以查询任何Consul服务器或任何Consul代理。 代理自动将查询转发到服务器。\n每个数据中心都运行Consul服务器集群。 当发生跨数据中心服务发现或配置请求时，本地Consul服务器将请求转发给远程数据中心并返回结果。\n简单使用 consul的使用非常简单，其采用Go语言编写，编译出的文件无其他依赖。因此只需要从官方下载对应平台的编译好的二进制可执行程序直接运行就可以了，下载地址https://www.consul.io/downloads.html\n安装 我使用的Linux系统，所以安装如下：\n1 curl -O https://releases.hashicorp.com/consul/1.0.2/consul_1.0.2_linux_amd64.zip \u0026amp;\u0026amp; sudo unzip consul_1.0.2_linux_amd64.zip -d /usr/local/bin/ 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 $ consul Usage: consul [--version] [--help] \u0026lt;command\u0026gt; [\u0026lt;args\u0026gt;] Available commands are: agent Runs a Consul agent catalog Interact with the catalog event Fire a new event exec Executes a command on Consul nodes force-leave Forces a member of the cluster to enter the \u0026#34;left\u0026#34; state info Provides debugging information for operators. join Tell Consul agent to join cluster keygen Generates a new encryption key keyring Manages gossip layer encryption keys kv Interact with the key-value store leave Gracefully leaves the Consul cluster and shuts down lock Execute a command holding a lock maint Controls node or service maintenance mode members Lists the members of a Consul cluster monitor Stream logs from a Consul agent operator Provides cluster-level tools for Consul operators reload Triggers the agent to reload configuration files rtt Estimates network round trip time between nodes snapshot Saves, restores and inspects snapshots of Consul server state validate Validate config files/directories version Prints the Consul version watch Watch for changes in Consul 启动 为了简单起见，我们将暂时在开发者模式中启动Consul代理。这个模式可以非常容易快速地启动一个单节点的Consul环境。当然它并不是用于生产环境下并且它也不会持久任何状态。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 $ consul agent -dev ==\u0026gt; Starting Consul agent... ==\u0026gt; Consul agent running! Version: \u0026#39;v1.0.2\u0026#39; Node ID: \u0026#39;14f11e8d-4904-3fcd-1cd5-251c311cf775\u0026#39; Node name: \u0026#39;master.soi.io\u0026#39; Datacenter: \u0026#39;dc1\u0026#39; (Segment: \u0026#39;\u0026lt;all\u0026gt;\u0026#39;) Server: true (Bootstrap: false) Client Addr: [127.0.0.1] (HTTP: 8500, HTTPS: -1, DNS: 8600) Cluster Addr: 127.0.0.1 (LAN: 8301, WAN: 8302) Encrypt: Gossip: false, TLS-Outgoing: false, TLS-Incoming: false ==\u0026gt; Log data will now stream in as it occurs: 2018/01/12 22:02:36 [DEBUG] Using random ID \u0026#34;14f11e8d-4904-3fcd-1cd5-251c311cf775\u0026#34; as node ID 2018/01/12 22:02:36 [INFO] raft: Initial configuration (index=1): [{Suffrage:Voter ID:14f11e8d-4904-3fcd-1cd5-251c311cf775 Address:127.0.0.1:8300}] 2018/01/12 22:02:36 [INFO] raft: Node at 127.0.0.1:8300 [Follower] entering Follower state (Leader: \u0026#34;\u0026#34;) 2018/01/12 22:02:36 [INFO] serf: EventMemberJoin: master.soi.io.dc1 127.0.0.1 2018/01/12 22:02:36 [INFO] serf: EventMemberJoin: master.soi.io 127.0.0.1 2018/01/12 22:02:36 [INFO] consul: Adding LAN server master.soi.io (Addr: tcp/127.0.0.1:8300) (DC: dc1) 2018/01/12 22:02:36 [INFO] consul: Handled member-join event for server \u0026#34;master.soi.io.dc1\u0026#34; in area \u0026#34;wan\u0026#34; 2018/01/12 22:02:36 [INFO] agent: Started DNS server 127.0.0.1:8600 (udp) 2018/01/12 22:02:36 [INFO] agent: Started DNS server 127.0.0.1:8600 (tcp) 2018/01/12 22:02:36 [INFO] agent: Started HTTP server on 127.0.0.1:8500 (tcp) 2018/01/12 22:02:36 [INFO] agent: started state syncer 2018/01/12 22:02:36 [WARN] raft: Heartbeat timeout from \u0026#34;\u0026#34; reached, starting election 2018/01/12 22:02:36 [INFO] raft: Node at 127.0.0.1:8300 [Candidate] entering Candidate state in term 2 2018/01/12 22:02:36 [DEBUG] raft: Votes needed: 1 2018/01/12 22:02:36 [DEBUG] raft: Vote granted from 14f11e8d-4904-3fcd-1cd5-251c311cf775 in term 2. Tally: 1 2018/01/12 22:02:36 [INFO] raft: Election won. Tally: 1 2018/01/12 22:02:36 [INFO] raft: Node at 127.0.0.1:8300 [Leader] entering Leader state 2018/01/12 22:02:36 [INFO] consul: cluster leadership acquired 2018/01/12 22:02:36 [INFO] consul: New leader elected: master.soi.io 2018/01/12 22:02:36 [DEBUG] consul: Skipping self join check for \u0026#34;master.soi.io\u0026#34; since the cluster is too small 2018/01/12 22:02:36 [INFO] consul: member \u0026#39;master.soi.io\u0026#39; joined, marking health alive 2018/01/12 22:02:36 [DEBUG] Skipping remote check \u0026#34;serfHealth\u0026#34; since it is managed automatically 2018/01/12 22:02:36 [INFO] agent: Synced node info 2018/01/12 22:02:36 [DEBUG] agent: Node info in sync 2018/01/12 22:02:37 [DEBUG] Skipping remote check \u0026#34;serfHealth\u0026#34; since it is managed automatically 2018/01/12 22:02:37 [DEBUG] agent: Node info in sync 集群成员 此时，打开另一个终端，使用consul members命令查看：\n1 2 3 $ consul members Node Address Status Type Build Protocol DC Segment master.soi.io 127.0.0.1:8301 alive server 1.0.2 2 dc1 \u0026lt;all\u0026gt; 使用server模式运行 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 $ consul agent -server -bootstrap -data-dir=$PWD -advertise=192.168.31.28 -client=0.0.0.0 -ui bootstrap = true: do not enable unless necessary ==\u0026gt; Starting Consul agent... ==\u0026gt; Consul agent running! Version: \u0026#39;v1.0.2\u0026#39; Node ID: \u0026#39;aac96fbf-2ccb-e0a8-2e82-aa4e3ff40d82\u0026#39; Node name: \u0026#39;master.soi.io\u0026#39; Datacenter: \u0026#39;dc1\u0026#39; (Segment: \u0026#39;\u0026lt;all\u0026gt;\u0026#39;) Server: true (Bootstrap: true) Client Addr: [0.0.0.0] (HTTP: 8500, HTTPS: -1, DNS: 8600) Cluster Addr: 192.168.31.28 (LAN: 8301, WAN: 8302) Encrypt: Gossip: false, TLS-Outgoing: false, TLS-Incoming: false ==\u0026gt; Log data will now stream in as it occurs: 2018/01/12 22:09:19 [INFO] raft: Initial configuration (index=1): [{Suffrage:Voter ID:aac96fbf-2ccb-e0a8-2e82-aa4e3ff40d82 Address:192.168.31.28:8300}] 2018/01/12 22:09:19 [INFO] raft: Node at 192.168.31.28:8300 [Follower] entering Follower state (Leader: \u0026#34;\u0026#34;) 2018/01/12 22:09:19 [INFO] serf: EventMemberJoin: master.soi.io.dc1 192.168.31.28 2018/01/12 22:09:19 [INFO] serf: EventMemberJoin: master.soi.io 192.168.31.28 2018/01/12 22:09:19 [INFO] consul: Handled member-join event for server \u0026#34;master.soi.io.dc1\u0026#34; in area \u0026#34;wan\u0026#34; 2018/01/12 22:09:19 [INFO] consul: Adding LAN server master.soi.io (Addr: tcp/192.168.31.28:8300) (DC: dc1) 2018/01/12 22:09:19 [INFO] agent: Started DNS server 0.0.0.0:8600 (udp) 2018/01/12 22:09:19 [INFO] agent: Started DNS server 0.0.0.0:8600 (tcp) 2018/01/12 22:09:19 [INFO] agent: Started HTTP server on [::]:8500 (tcp) 2018/01/12 22:09:19 [INFO] agent: started state syncer 在浏览器中打开http://localhost:8500\n","date":"2018-01-07T19:38:00+08:00","image":"https://ronggle.com/2018/consul-first-time/consul-ui_hu73244f513327a036976c582a5b438755_67330_120x120_fill_box_smart1_3.png","permalink":"https://ronggle.com/2018/consul-first-time/","title":"consul学习一-初见"},{"content":"生活计划 每周至少打一次球 坚持每天锻炼身体 坚持23点睡觉 读书计划 《Kubernetes权威指南》 《开源容器云-OpenShift》 《Vue权威指南》 《领域驱动设计》 《微服务架构与实践》 课程学习 CKA 技术学习 Consul Golang Spring Boot/Cloud Nomad CI/CD(CDS,GoCD,Jenkins) OpenID Connect K8s Docker Nodejs Python 项目计划 onion 其他计划 坚持写博客 坚持写博客 坚持写博客 ","date":"2018-01-04T21:48:48+08:00","image":"https://ronggle.com/2018/2018-technology-plan/2018_hu4f0e4a918025a1f9bb615bc46e5430c4_81468_120x120_fill_q75_box_smart1.jpg","permalink":"https://ronggle.com/2018/2018-technology-plan/","title":"2018年计划"},{"content":"不搭的前言 而今，开发的项目庞大而且复杂，涉及多个模块，跨多个项目组，如果再按照口头沟通协调，很难想象是多么艰难。而在日常的一些开发过程中，我们也需要不停的对自己开发的代码进行调试，比如某些公司，有些产品线根本没有稳定的开发环境提供给开发人员使用，那么，怎么办，只能使用mock的方式。\n说到mock，其实我也再使用，有时候，不能每时每刻都调用真实的服务，我们就会再自己项目中设置开关，当再开发环境下时，使用自己再本地文件中伪造的数据，这些文件有JSON、TXT、CSV/EXCEL，往往再不同的模块和借口会写很多不同的Mock数据处理的逻辑，没有复用性，且没有良好的版本控制与管理。\n那么要怎么处理这些Mock数据呢？今天我们来试玩一下Easy Mock。\n什么是Easy Mock 伪造数据，我们更高效；但，不仅于此。\n这是Easy Mock的官网上面的介绍。\nEasy Mock是一个极其简单、高效、可视化、并且能快速生成模拟数据的在线Mock服务。以项目管理的方式组织Mock数据，能帮助我们更好的管理Mock数据，不怕丢失。\n特性 支持接口代理 支持快捷键操作 支持协同编辑 支持团队项目 支持 Restful 支持 Swagger 1.2 \u0026amp; 2.0 基于 Swagger 快速创建项目 支持显示接口入参与返回值 支持显示实体类 支持灵活性与扩展性更高的响应式数据开发 支持 Mock.js 语法 支持 restc 方式的接口预览 在线使用文档\nEasy Mock CLI - 基于 Easy Mock 快速生成 api.js 的命令行工具。\n数据伪造 我前面也提到过，我们才有的是编程式的伪造数据，利用请求拦截或者配置文件开关，来切换我们的正常数据请求与Mock数据。\n这些方法虽然可以解决我们的问题，但同样伴随着一些问题，比如说，依赖特定的框架，主要体现再我们使用spring的配置文件profile作为切换开关；又比如，脏代码，主要是我们会带项目源码中以编程的方式伪造Mock数据；又比如说，这些数据没法统一管理，变更困难等等。his\n说到以上的这些困难，可能还是没能触及你的痛点，但是无所谓了，哈哈哈，接下来，我们试试Easy Mock吧。\n安装 在开始之前，假设你已经成功安装了 Node.js (v7.4 以上) 和 MongoDB (v3.4 以上)\n注意，在安装项目依赖时，bcrypt需要c++编译环境g++，如果你没有安装，那需要自行安装\n$ sudo dnf install gcc-c++ # 注意，本屌使用的是Fedora，你需要根据自己的操作系统找到安装方式\n1 2 $ git clone https://github.com/easy-mock/easy-mock.git $ cd easy-mock \u0026amp;\u0026amp; npm install 配置 不同环境会加载不同的配置文件，在此之前你应该对 node-config 有所了解。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 $ pwd /home/soi/web/easy-mock $ cd config $ cat default.json { \u0026#34;port\u0026#34;: 7300, \u0026#34;pageSize\u0026#34;: 30, \u0026#34;routerPrefix\u0026#34;: { \u0026#34;mock\u0026#34;: \u0026#34;/mock\u0026#34;, \u0026#34;api\u0026#34;: \u0026#34;/api\u0026#34; }, \u0026#34;db\u0026#34;: \u0026#34;mongodb://192.168.31.28:27017/easy-mock\u0026#34;, ... } 注意，这里我们需要配置的是MongoDB，可以不配置其他的。如果你需要一些高级用法，你可以参考github仓库的文档进行配置。\n启动 1 $ npm run dev 这里使用的开发模式启动，可以在你的浏览器中访问http://127.0.0.1:7300\n更多命令：\n$ npm run build # 前端静态资源构建打包\n$ npm run start # 以生产环境方式启动，需要提前执行 build\n$ npm run test # 测试\n$ npm run lint # 语法检测\n使用 登录页 注意，这里如果你没有用户名和密码的话，直接输入用户名密码，会给新建一个用户\n个人项目 登录成功后，进入我的个人项目面板。\n点击项目可以查看项目接口列表。\n团队项目 首先，我们来初始化一个团队。 新建团队项目 编辑项目API 预览项目API 后语 这里我就水完了一篇博客了，对你有没有帮助我不知道。\n不过首先的感谢下大搜车无线架构团队给我们开源这个好用的Mock神器。\n","date":"2017-09-03T12:38:11+08:00","image":"https://ronggle.com/2017/easy-mock-getting-and-started/my-project_huda48920455a3f353154d1a2a68d6945c_69133_120x120_fill_box_smart1_3.png","permalink":"https://ronggle.com/2017/easy-mock-getting-and-started/","title":"Easy Mock 试玩"},{"content":"再次安装Linux是两年前，自己安装了个Ubuntu，后面安装了Deepin，总体上感觉都很不错，特别是Deepin，国内发行版算是颜值比较高的了，一直都很喜欢。\n直到去年我的笔记本报废，六七年前的低配电脑，实在扛不住了。在换了新的电脑后，i7、16G、GTX960，很给力的配置，但是在我装Ubuntu的过程中，严重不喜欢这个配置了，显卡不兼容，上面的Windows10还特别讨厌，不得不放弃。\n其实在很久之前，就听说Fedora很不错，最近因为折腾Openshift，突发奇想，安装了一个，然后被Fedora的颜值（肯定是桌面）吸引。\n一张美美的桌面\n应用列表\n弹出\n多窗口\n重命名\n终端 漂亮不？如果你也喜欢，安装个试试呗！\n","date":"2017-08-03T21:39:17+08:00","image":"https://ronggle.com/2017/fedora/home_hu4149d0921973420729ebb226d45329e8_1405605_120x120_fill_box_smart1_3.png","permalink":"https://ronggle.com/2017/fedora/","title":"Fedora 桌面系统体验"},{"content":"在一次次的游戏中，有胜利，有失败，也有愤怒，也有责怪，到最后，我还是永恒的“青铜”。\n扯远了，回到主题，前几天和小伙伴一起打排位，有生之年，最舒服的一次排位。从而有了下面的这些思考，纯属个人观点，不喜勿喷！额，其实也不会有人喷的。\n团队 三个臭皮匠，顶个诸葛亮。不得不说，在我们的日常生活中，会见到很多团队，有技术的，有施工的，各有不同。\n在王者农药中，选择一场对局，配合队友，选择自己擅长的英雄，射手、法师、打野、辅助、肉盾，各司其职。\n在游戏的过程中，一般都会根据英雄的性质不同，选择不同的路线。同时，要准备好随时支援处于危险中的队友。\n当然，实际上这个是理想的状态。\n人都是自私的，所谓，“人不为己，天诛地灭”。很多时候，组建的团队很多时候都是残缺不全的。能够做到无私奉献自己的，能有多少人呢？我们都是凡夫俗子，都有七情六欲。\n我记得以前有一局，我选择了法师，我们5人一起推中路，这时候，对面4个人过来了，我的上去就是一个大，结果，我的队友全走了，对没错，走了，留我一个人在那里懵逼。\n在后面的回合中，我幸苦的打残血了一个，这时候我的两个队友勇敢的上去，追着人家残血不放，各种喊撤退都不停，从中路一路追到下路，然后中了埋伏。不是，我说你们这个何必呢？\n还有一次，对面两个肉盾在前面扛着，后面输出贼猛，我的队友却一心一意，很认真很认真的打肉盾。咱能不逗吗？\n其实，在实际的生活中，我们工作的团队也会出现这样的情况，当一个任务来了，很多人都会避而不及，当一点好处来了，很多人都会弃队友于不顾，自己先上。\n往往在遇到问题的时候，总是想办法把锅丢出去。\n我记得我以前在某公司，当时发生了故障，所有人都在找到底是谁造成的。其实，这个时候重要的不是找到对策，解决这个问题吗？为啥都在找是谁造成的呢？\n在一局农药中，我是一个快乐的小射手，辅助我的哥们在草丛里面看着我和对面怼，没错，看着，当我把对面怼残血的时候，他一个漂亮的技能，抢到一个人头。没错，我还站在哪里蒙圈，第二次还这样，第三次也是。不得不承认，当时我很想祝贺他。\n在我理想状态下的团队，应该是一个荣辱与共的。正好今天听到一句台词，“你们没有荣耀，荣耀只属于遵守纪律的团队，不属于你们这样的，一团散沙的队伍”。\n定位 定位，也就是我们要选择适合自己的角色，并且，尽可能发挥自己，配合队友。比如，当我选择做一名肉盾的时候，我会尽可能拖住对面，保护后排的队友；如果我选择了输出，我会尽可能先朝着对方输出，在我的队友倒下前，尽可能让他们受到最短时间的伤害；当作为一个辅助时，需要让队友得到最多的资源。\n其实生活中也一样，在一个团队里，不管你是老人，还是新人，如果自己才华不如队友，就尽可能辅助队友，让他绽放自己的光彩，而在某些领域，自己能够胜任的，一定要尽心去完成。\n当然，这其中还有个重要的角色，就是为队伍发号施令的人，什么们时候改前进，什么时候该撤退。而这个人，必须能够关注到战场的形势，及时调整战术，对应瞬息万变的战场。再一次任务失败后，作出最佳的调整，鼓励队友，下一次再战。\n最后 好吧，我承认，我是标题党。\n我说了一堆废话，其实是希望，一个团队，应该拥有自己的荣耀。\n","date":"2017-07-29T17:37:14+08:00","image":"https://ronggle.com/2017/a-game-of-thinking/game_hu54851f999d21ed6f60405180e9c05051_111211_120x120_fill_box_smart1_3.png","permalink":"https://ronggle.com/2017/a-game-of-thinking/","title":"一场王者农药引发的思考"},{"content":"将应用程序用于向Zipkin报告时序数据。 Zipkin UI还提供了一个依赖关系图，显示了每个应用程序中跟踪的请求数量。 如果要解决延迟问题或错误，可以根据应用程序，跟踪长度，注释或时间戳来过滤或排序所有跟踪。 选择跟踪后，您可以看到每个跨度所需的总跟踪时间的百分比，从而允许您识别问题应用程序。\n快速开始 首先，我是一个spring cloud的初学者，所以我在本次采用了spring cloud来学习zipkin，当然，只要你愿意，你完全可以不采用spring cloud。\n项目结构如下：\n1 2 3 4 5 6 7 - platform - account - config - registry - zipkin - zuul pom.xml 这是个多模块项目，其中子模块分别对应的是账号服务、配置中心、注册中心、zipkin服务、API Getaway。\nZipkin Server 首先，我们来看看zipkin模块的pom.xml\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 \u0026lt;?xml version=\u0026#34;1.0\u0026#34; encoding=\u0026#34;UTF-8\u0026#34;?\u0026gt; \u0026lt;project xmlns=\u0026#34;http://maven.apache.org/POM/4.0.0\u0026#34; xmlns:xsi=\u0026#34;http://www.w3.org/2001/XMLSchema-instance\u0026#34; xsi:schemaLocation=\u0026#34;http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd\u0026#34;\u0026gt; \u0026lt;parent\u0026gt; \u0026lt;artifactId\u0026gt;platform\u0026lt;/artifactId\u0026gt; \u0026lt;groupId\u0026gt;team.soi\u0026lt;/groupId\u0026gt; \u0026lt;version\u0026gt;1.0-SNAPSHOT\u0026lt;/version\u0026gt; \u0026lt;/parent\u0026gt; \u0026lt;modelVersion\u0026gt;4.0.0\u0026lt;/modelVersion\u0026gt; \u0026lt;artifactId\u0026gt;zipkin\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;1.0-SNAPSHOT\u0026lt;/version\u0026gt; \u0026lt;dependencies\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.cloud\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-cloud-starter-config\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.cloud\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-cloud-starter-eureka\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.boot\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-boot-starter-actuator\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;io.zipkin.java\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;zipkin-server\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;io.zipkin.java\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;zipkin-autoconfigure-ui\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;io.zipkin.java\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;zipkin-autoconfigure-storage-mysql\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;mysql\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;mysql-connector-java\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.boot\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-boot-starter-jdbc\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.projectlombok\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;lombok\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.jetbrains.kotlin\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;kotlin-stdlib\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;/dependencies\u0026gt; \u0026lt;build\u0026gt; \u0026lt;finalName\u0026gt;zipkin\u0026lt;/finalName\u0026gt; \u0026lt;plugins\u0026gt; \u0026lt;plugin\u0026gt; \u0026lt;groupId\u0026gt;org.apache.maven.plugins\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;maven-compiler-plugin\u0026lt;/artifactId\u0026gt; \u0026lt;/plugin\u0026gt; \u0026lt;plugin\u0026gt; \u0026lt;artifactId\u0026gt;kotlin-maven-plugin\u0026lt;/artifactId\u0026gt; \u0026lt;groupId\u0026gt;org.jetbrains.kotlin\u0026lt;/groupId\u0026gt; \u0026lt;/plugin\u0026gt; \u0026lt;plugin\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.boot\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-boot-maven-plugin\u0026lt;/artifactId\u0026gt; \u0026lt;/plugin\u0026gt; \u0026lt;/plugins\u0026gt; \u0026lt;/build\u0026gt; \u0026lt;/project\u0026gt; 从pom.xml中，不难看出，我引入了zipkin-server、zipkin-autoconfigure-ui、zipkin-autoconfigure-storage-mysql，在本例中，我采用的mysql存储。\n接下来，我们只需要在启动类中打开@EnableZipkinServer注解就OK。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 package team.soi.platform.zipkin import org.springframework.boot.SpringApplication import org.springframework.boot.autoconfigure.SpringBootApplication import org.springframework.cloud.client.discovery.EnableDiscoveryClient import zipkin.server.EnableZipkinServer /** * @author Soi. * * @version 1.0 * * @see */ @SpringBootApplication @EnableZipkinServer @EnableDiscoveryClient open class App fun main(args: Array\u0026lt;String\u0026gt;) { SpringApplication.run(App::class.java, *args) } zipkin服务的配置文件application.yml如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 server: port: ${port:9411} spring: application: name: zipkin datasource: schema: classpath:/mysql.sql url: jdbc:mysql://localhost:3306/zipkin?autoReconnect=true username: root password: root driver-class-name: com.mysql.jdbc.Driver initialize: true continue-on-error: true eureka: client: serviceUrl: defaultZone: ${defaultZone:http://localhost:8761/eureka/} Zipkin Client 在使用zipkin服务的模块的pom.xml中加入下面的依赖：\n1 2 3 4 \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.cloud\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-cloud-starter-zipkin\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; 然后在application.yml中注明zipkin的url：\n1 2 3 spring: zipkin: base-url: http://localhost:9411 在我的示例中，我分别在config、zuul、account模块中加入了zipkin的配置。\nZipkin UI 首先，我分别启动了这几个模块，我们可以在Eureka的UI中看到：\n然后我分别在浏览器中访问API Getaway中映射的APIhttp://localhost:9200/account/accounts/1和http://localhost:9200/config/platform/zuul/dev，然后在打开Zipkin页面。\n我们可以看到服务调用的依赖关系：\n还可以看到调用的详细信息：\n总结 在spring cloud中，为我们集成了许许多多工具，为我们的微服务开发提供了便利，同时也解决了我们的一些痛点，而Zipkin，为我们解决了服务调用链的追踪问题，同时提供了详细的调用时间，可以是我们更加有效地知晓服务的性能瓶颈在哪些地方。\n当然，本例只是简单的介绍了下Zipkin的使用，没有想过会给你带来什么。\n","date":"2017-07-22T23:30:22+08:00","image":"https://ronggle.com/2017/what-is-zipkin/index_hu1d979cfaae9601218fa80bc3c4f22156_37994_120x120_fill_box_smart1_3.png","permalink":"https://ronggle.com/2017/what-is-zipkin/","title":"什么是Zipkin"},{"content":"前言 在过去的不知道多少年前，我们的应用和数据库，还部署在一台机器上，大概如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 +----------+ +--------------+ | | | | | | | | | | | +--------+ | | +--------------------\u0026gt; | | | | | | | App | | | | \u0026lt;------------------+ | ^ + | | | Client | | +--------+ | | | | | | | | | | +--------+ | | | | | + v | | | | | | DB | | | | | | | | | | | +--------+ | | | | | | | | Server | +----------+ +--------------+ 随着业务的增长，这样的架构已经不能满足大量的请求，这时候架构也随之改变，把应用与数据库拆分开来：\n1 2 3 4 5 6 7 +--------+ +---------+ +-------+ | | | | | | | +-------------\u0026gt; | +-----------\u0026gt; | | | Client | | App | | DB | | | \u0026lt;-------------+ | \u0026lt;-----------+ | | | | | | | +--------+ +---------+ +-------+ 当然，业务是不停的增长的，而上述这样的也会发生变化，可以把应用与数据库进行集群部署。\n然后\u0026hellip;\n不搭的后语 而如今，技术与架构，在业务的驱动下，早已发生了巨大的变化。\n在最近两年，最火的，莫过于服务化，容器化与云化。\n微服务 微服务架构，它把软件围绕业务功能结构进行划分、拆解成服务，以服务构建应用，而服务可以独立部署、独立扩展，服务也可以提供模块化的边界，并且不同的使用也可以使用不同的开发语言。\n微服务风格的特性：\n组件与服务 围绕业务功能进行组织 产品不是项目 强化终端及弱化通道 分散治理 分散数据管理 基础设施自动化 容错性设计 设计改进 容器 容器，另一个热门的话题，而在这个话题中，我们能够快速想到的，应该是Docker。\nDocker 最初是 dotCloud 公司创始人 Solomon Hykes 在法国期间发起的一个公司内部项目，它是基于 dotCloud 公司多年云服务技术的一次革新，并于 2013 年 3月以 Apache 2.0 授权协议开源。\nDocker包括三个基本概念镜像、容器、仓库。镜像和容器的关系，就像是面向对象程序设计中的类和实例一样，镜像是静态的定义，容器是镜像运行时的实例；容器可以被创建、启动、停止、删除、暂停等；而仓库，则是用来统一存储镜像，进行版本控制。\n使用容器技术，带来了以下好处：\n持续部署与测试 跨云平台支持 环境标准化和版本控制 高资源利用率与隔离 简单易用 \u0026hellip; 容器云 容器云是以容器为资源分割和调度的基本单位，封装整个软件运行时环境，为开发者和系统管理员提供用于构建、发布和运行分布式运用的平台。\n当容器云专注于资源共享与隔离、容器编排与部署时，它更接近传统的IaaS；当容器云渗透到应用支撑与运行时环境时，它更接近传统的PaaS。\n使用容器云，带来了以下好处：\n更高效的利用系统资源 更快速的启动时间 一致的运行环境 持续交付和部署 更轻松的迁移 更轻松的维护和扩展。 服务化，容器化与云化 当软件服务化后，我们的架构看起来应该是这样的：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 +-------+ +--------------------------------------+ +--------------------------+ | | | | | | | | | WebUI | | App | | | | | | | | | +--------------------------------------+ ---------------------------+ | | | | +-------------------------------------------------------------------+ | | | | | | | RESTful API | | | +-------------------------------------------------------------------+ | | | | +-----------+ +-----------------------------------+ +-------------+ | | | | | +-----------+ +------------+ | | | |Monitor| | Registry | | | Service1 | | Service2 | | | Configure | | | | \u0026amp; | | +-----------+ +------------+ | | | | | | Dscovery | | | | | | | | | | +-----------+ | | | | | | | | | ... | Services | | | | | | | | +-----------+ | | | | | | | | | | | | | | | | | | | | | +-----------+ +-----------------------------------+ +-------------+ | | | | +-------------------------------------------------------------------+ | | | +-------------+ +------------+ +----------+ | | | | | DB | | MQ | | ... | | | | | +-------------+ +------------+ +----------+ | +-------+ +-------------------------------------------------------------------+ 众多的服务给运维带来极大的挑战，要求能够快速地发布与容错处理，而如果再使用人肉运维的方式再也无法满足。\n这时候，一个借助于CI/CD的研发流水线的开发方式，渐渐进入了视野：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 +--------------+ +------------+ Re^iew +--------------+ | | | +-------\u0026gt; | | | Developer | Push | | | Maintainer | | +--------\u0026gt; | Repo | Merge | | | | | | \u0026lt;----+ | | +------+-------+ +-----+------+ +--------------+ ^ | | | | v Trigger | | +------------+ Feedback | | | CI | | CD | +----------+ CI/CD +-------------+ | | | | | | v +------------+ v | +-------------------------------+ +--------------------------------------+ |-------+ +-------+ +--------+ | | | ||Build +-\u0026gt; Unit +-\u0026gt;Analyze +-+| | +---------+ +---------+ +---------+ | |-------+ +-------+ +--------+ || | |Test Env | |UAT Env | |Production | |-------+ +-------+ +--------+ || | | | | | | | | ||Deploy\u0026lt;-+Package\u0026lt;-+ Test \u0026lt;-+| | +---------+ +---------+ +---------+ | |-------+ +-------+ +--------+ | | | +-------------------------------+ +--------------------------------------+ 可以借助容器与容器云，将服务进行持续集成、持续部署与交付，最终达到自动化。\n放弃 服务化是未来吗？容器是未来吗？容器云是未来吗？\n谁知道呢？未来，还没来！\n只是现在，作为一枚面向搜索引擎复制粘贴的开发者，在不停的学习中，开拓自己的视野，积累自己的经验，为后来的业务需求打下坚实的基础也是势在必行。\n参考：\n微服务(Microservices)\n《Docker——容器与容器云》试读：1.3　进化：从容器到容器云\n","date":"2017-05-22T21:26:33+08:00","permalink":"https://ronggle.com/2017/microservice-container-and-container-cloud/","title":"微服务、容器与容器云-从入门到放弃"},{"content":"WebService是一个平台独立的，低耦合的，自包含的、基于可编程的web的应用程序，可使用开放的XML（标准通用标记语言下的一个子集）标准来描述、发布、发现、协调和配置这些应用程序，用于开发分布式的互操作的应用程序。 巴拉巴拉。。。。。。\n开始编写接口 1 2 3 4 5 6 7 8 9 10 11 package team.soi.service; public interface HelloService { /** * to do sth. * @param to * @return */ Object toDoSth(String to); } 编写接口的实现类 1 2 3 4 5 6 7 8 9 10 11 12 13 package team.soi.service.impl; import team.soi.service.HelloService; import javax.jws.WebService; @WebService public class HelloServiceImpl implements HelloService { public Object toDoSth(String to) { return \u0026#34;Hello,\u0026#34; + to + \u0026#34;! Welcome to my webservice world!\u0026#34;; } } 发布WebService 1 2 3 4 5 6 7 8 9 10 11 package team.soi; import team.soi.service.impl.HelloServiceImpl; import javax.xml.ws.Endpoint; public class App { public static void main(String[] args) { Endpoint.publish(\u0026#34;http://localhost:8899/ws/demo\u0026#34;, new HelloServiceImpl()); } } 奏是介么仍性，在浏览器输入http://localhost:8899/ws/demo?wsdl 见证奇迹的时刻到了，Duang\u0026hellip;\u0026hellip;如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 This XML file does not appear to have any style information associated with it. The document tree is shown below. \u0026lt;!-- Published by JAX-WS RI (http://jax-ws.java.net). RI\u0026#39;s version is JAX-WS RI 2.2.9-b130926.1035 svn-revision#5f6196f2b90e9460065a4c2f4e30e065b245e51e. --\u0026gt; \u0026lt;!-- Generated by JAX-WS RI (http://jax-ws.java.net). RI\u0026#39;s version is JAX-WS RI 2.2.9-b130926.1035 svn-revision#5f6196f2b90e9460065a4c2f4e30e065b245e51e. --\u0026gt; \u0026lt;definitions xmlns:wsu=\u0026#34;http://docs.oasis-open.org/wss/2004/01/oasis-200401-wss-wssecurity-utility-1.0.xsd\u0026#34; xmlns:wsp=\u0026#34;http://www.w3.org/ns/ws-policy\u0026#34;xmlns:wsp1_2=\u0026#34;http://schemas.xmlsoap.org/ws/2004/09/policy\u0026#34; xmlns:wsam=\u0026#34;http://www.w3.org/2007/05/addressing/metadata\u0026#34; xmlns:soap=\u0026#34;http://schemas.xmlsoap.org/wsdl/soap/\u0026#34;xmlns:tns=\u0026#34;http://impl.service.soi.team/\u0026#34; xmlns:xsd=\u0026#34;http://www.w3.org/2001/XMLSchema\u0026#34; xmlns=\u0026#34;http://schemas.xmlsoap.org/wsdl/\u0026#34; targetNamespace=\u0026#34;http://impl.service.soi.team/\u0026#34;name=\u0026#34;HelloServiceImplService\u0026#34;\u0026gt; \u0026lt;types\u0026gt; \u0026lt;xsd:schema\u0026gt; \u0026lt;xsd:import namespace=\u0026#34;http://impl.service.soi.team/\u0026#34; schemaLocation=\u0026#34;http://localhost:8899/ws/demo?xsd=1\u0026#34;/\u0026gt; \u0026lt;/xsd:schema\u0026gt; \u0026lt;/types\u0026gt; \u0026lt;message name=\u0026#34;toDoSth\u0026#34;\u0026gt; \u0026lt;part name=\u0026#34;parameters\u0026#34; element=\u0026#34;tns:toDoSth\u0026#34;/\u0026gt; \u0026lt;/message\u0026gt; \u0026lt;message name=\u0026#34;toDoSthResponse\u0026#34;\u0026gt; \u0026lt;part name=\u0026#34;parameters\u0026#34; element=\u0026#34;tns:toDoSthResponse\u0026#34;/\u0026gt; \u0026lt;/message\u0026gt; \u0026lt;portType name=\u0026#34;HelloServiceImpl\u0026#34;\u0026gt; \u0026lt;operation name=\u0026#34;toDoSth\u0026#34;\u0026gt; \u0026lt;input wsam:Action=\u0026#34;http://impl.service.soi.team/HelloServiceImpl/toDoSthRequest\u0026#34; message=\u0026#34;tns:toDoSth\u0026#34;/\u0026gt; \u0026lt;output wsam:Action=\u0026#34;http://impl.service.soi.team/HelloServiceImpl/toDoSthResponse\u0026#34; message=\u0026#34;tns:toDoSthResponse\u0026#34;/\u0026gt; \u0026lt;/operation\u0026gt; \u0026lt;/portType\u0026gt; \u0026lt;binding name=\u0026#34;HelloServiceImplPortBinding\u0026#34; type=\u0026#34;tns:HelloServiceImpl\u0026#34;\u0026gt; \u0026lt;soap:binding transport=\u0026#34;http://schemas.xmlsoap.org/soap/http\u0026#34; style=\u0026#34;document\u0026#34;/\u0026gt; \u0026lt;operation name=\u0026#34;toDoSth\u0026#34;\u0026gt; \u0026lt;soap:operation soapAction=\u0026#34;\u0026#34;/\u0026gt; \u0026lt;input\u0026gt; \u0026lt;soap:body use=\u0026#34;literal\u0026#34;/\u0026gt; \u0026lt;/input\u0026gt; \u0026lt;output\u0026gt; \u0026lt;soap:body use=\u0026#34;literal\u0026#34;/\u0026gt; \u0026lt;/output\u0026gt; \u0026lt;/operation\u0026gt; \u0026lt;/binding\u0026gt; \u0026lt;service name=\u0026#34;HelloServiceImplService\u0026#34;\u0026gt; \u0026lt;port name=\u0026#34;HelloServiceImplPort\u0026#34; binding=\u0026#34;tns:HelloServiceImplPortBinding\u0026#34;\u0026gt; \u0026lt;soap:address location=\u0026#34;http://localhost:8899/ws/demo\u0026#34;/\u0026gt; \u0026lt;/port\u0026gt; \u0026lt;/service\u0026gt; \u0026lt;/definitions\u0026gt; 到这里，说明你的WebService服务端编写完成了，接下来，我们要怎么去调用呢？ 说真的，在今天之前，我就知道WebService可以如上的写法，但是不会调用，感觉自己好Low，于是乎，在群里面一吼，很多热心的人回答了我的问题，得知了接下来该做的事情。\n生成Jar 在你的终端里，输入命令，如下：\n1 2 3 4 5 6 7 8 9 10 11 soi@soi:~/workspace/wsc$ wsimport -extension -keep -p team.soi.ws.client -s ./src -d ./bin http://localhost:8899/ws/demo?wsdl 正在解析 WSDL... 正在生成代码... 正在编译代码... soi@soi:~/workspace/wsc$ cd bin soi@soi:~/workspace/wsc/bin$ jar cvf hello-ws-demo.jar team soi@soi:~/workspace/wsc/bin$ ls hello-ws-demo.jar team 此时，我们已经拿到了本WebService的客户端jar包，我们将客户端jar包加入到我们的工程，顺理成章，开始编写客户端代码：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 package team.soi; import junit.framework.TestCase; import team.soi.ws.client.HelloServiceImpl; import team.soi.ws.client.HelloServiceImplService; public class AppTest extends TestCase { /** * test ws */ public void testWs() { HelloServiceImpl service = new HelloServiceImplService().getHelloServiceImplPort(); String s = (String) service.toDoSth(\u0026#34;Soi\u0026#34;); System.out.println(s); } } 运行测试代码：\n1 Hello,Soi! Welcome to my webservice world! 撸完收工\u0026hellip;\u0026hellip;.\n","date":"2017-02-12T19:32:45+08:00","permalink":"https://ronggle.com/2017/webservice-hello-world/","title":"WebService学习记录"},{"content":"“我来到 你的城市 走过你来时的路 想像着 没我的日子 你是怎样的孤独”\u0026mdash;陈奕迅《好久不见》\n每次看着这张图，我都会莫名的悲伤。看着这孤独的背影，倍感凄凉。\n初识，在那涂炭的血池，魔君肆虐，你我携手共进。\n不知为何，我竟如此这般迷恋于你，你的可爱与天真，烂漫了我的世界，甚至使其崩塌。\n我向你表达了我的爱慕，你没有拒绝，三生石旁，我许你三生七世，若有负于你，愿永堕阎罗。\n我不曾为你准备什么，只是结发，成为夫妻，没有婚礼，没人宾客，你我相拥凝望。\n从此江湖，你我相伴，我们去过星界，去过中州，去过西陆，去过雪国，去过沧海、大荒、荧惑岛，在北部雪国是一片冰雪荒芜之地，我们看着雪白的世界，忘却了流离的生活。\n仙魔大战，你为了守护我，甘愿与我同为魔族，从此走上黑暗，而我，在不停的战斗中辗转，你只是静静的守候着我，什么都不说，偶尔停下来，我凝望着你的双眸，泛着忧伤。\n而如今，我强大到可以独挡一面，你却不知去向。\n没有你，我戎马半生，得到天下又能怎么样？\n我独自一人，去过我们去过的每一个地方，看我们看过的每一处美景，却只能感受到孤独和忧伤。\n或许，我们此生，都不会再见，而我，注定孤独\u0026hellip;..\n每一天活在思念的池子里，游不出来，又或者不愿意出来\u0026hellip;..\n如果天黑之前来得及，我要忘了你眼睛，静静等候，余下的七世三生!\n","date":"2015-10-24T21:26:33+08:00","image":"https://ronggle.com/2015/it-is-been-a-long-time/1_hudca41cb8c7bbd2d1490a6c94ff30931c_52404_120x120_fill_q75_box_smart1.jpg","permalink":"https://ronggle.com/2015/it-is-been-a-long-time/","title":"好久不见"}]